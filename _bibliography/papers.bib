---
---

@article{gan_threedworld:_2020,
 abstract = {We introduce ThreeDWorld (TDW), a platform for interactive multi-modal physical simulation. TDW enables simulation of high-fidelity sensory data and physical interactions between mobile agents and objects in rich 3D environments. Unique properties include: real-time near-photo-realistic image rendering; a library of objects and environments, and routines for their customization; generative procedures for efficiently building classes of new environments; high-fidelity audio rendering; realistic physical interactions for a variety of material types, including cloths, liquid, and deformable objects; customizable agents that embody AI agents; and support for human interactions with VR devices. TDW's API enables multiple agents to interact within a simulation and returns a range of sensor and physics data representing the state of the world. We present initial experiments enabled by TDW in emerging research directions in computer vision, machine learning, and cognitive science, including multi-modal physical scene understanding, physical dynamics predictions, multi-agent interactions, models that learn like a child, and attention studies in humans and neural networks.},
 author = {Chuang Gan and Jeremy Schwartz and Seth Alter and Damian Mrowca and Martin Schrimpf and James Traer and Julian De Freitas and Jonas Kubilius and Abhishek Bhandwaldar and Nick Haber and Megumi Sano and Kuno Kim and Elias Wang and Michael Lingelbach and Aidan Curtis and Kevin Feigelis and Daniel M Bear and Dan Gutfreund and David Cox and Antonio Torralba and James J DiCarlo and Joshua B Tenenbaum and Josh H McDermott and Daniel LK Yamins},

 journal = {arXiv preprint arXiv:2007.04954},
 year = {2020},
 title = {Threedworld: A platform for interactive multi-modal physical simulation}
}


@article{duda_use_2016,
 abstract = {Although autism spectrum disorder (ASD) and attention deficit hyperactivity disorder (ADHD) continue to rise in prevalence, together affecting> 10% of today’s pediatric population, the methods of diagnosis remain subjective, cumbersome and time intensive. With gaps upward of a year between initial suspicion and diagnosis, valuable time where treatments and behavioral interventions could be applied is lost as these disorders remain undetected. Methods to quickly and accurately assess risk for these, and other, developmental disorders are necessary to streamline the process of diagnosis and provide families access to much-needed therapies sooner. Using forward feature selection, as well as undersampling and 10-fold cross-validation, we trained and tested six machine learning models on complete 65-item Social Responsiveness Scale score sheets from 2925 individuals with either ASD (n= 2775) or ADHD …},
 author = {M Duda and R Ma and N Haber and DP Wall},

 journal = {Translational psychiatry},
 number = {2},
 pages = {e732-e732},
 year = {2016},
 publisher = {Nature Publishing Group},
 title = {Use of machine learning for behavioral distinction of autism and ADHD},
 volume = {6}
}


@article{mrowca_flexible_2018,
 abstract = {Humans have a remarkable capacity to understand the physical dynamics of objects in their environment, flexibly capturing complex structures and interactions at multiple levels of detail.},
 author = {Damian Mrowca and Chengxu Zhuang and Elias Wang and Nick Haber and Li F Fei-Fei and Josh Tenenbaum and Daniel L Yamins},

 journal = {Advances in Neural Information Processing Systems},
 year = {2018},
 title = {Flexible neural representation for physics prediction},
 volume = {31}
}


@article{voss_effect_2019,
 abstract = {Autism behavioral therapy is effective but expensive and difficult to access. While mobile technology–based therapy can alleviate wait-lists and scale for increasing demand, few clinical trials exist to support its use for autism spectrum disorder (ASD) care.To evaluate the efficacy of Superpower Glass, an artificial intelligence–driven wearable behavioral intervention for improving social outcomes of children with ASD.A randomized clinical trial in which participants received the Superpower Glass intervention plus standard of care applied behavioral analysis therapy and control participants received only applied behavioral analysis therapy. Assessments were completed at the Stanford University Medical School, and enrolled participants used the Superpower Glass intervention in their homes. Children aged 6 to 12 years with a formal ASD diagnosis who were …},
 author = {Catalin Voss and Jessey Schwartz and Jena Daniels and Aaron Kline and Nick Haber and Peter Washington and Qandeel Tariq and Thomas N Robinson and Manisha Desai and Jennifer M Phillips and Carl Feinstein and Terry Winograd and Dennis P Wall},

 journal = {JAMA pediatrics},
 number = {5},
 pages = {446-454},
 year = {2019},
 publisher = {American Medical Association},
 title = {Effect of wearable digital intervention for improving socialization in children with autism spectrum disorder: a randomized clinical trial},
 volume = {173}
}


@article{haber_learning_2018,
 abstract = {Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to mathematically formalize these abilities using a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulated environment in which an agent can move and interact with objects it sees, we propose a" world-model" network that learns to predict the dynamic consequences of the agent's actions. Simultaneously, we train a separate explicit" self-model" that allows the agent to track the error map of its world-model. It then uses the self-model to adversarially challenge the developing world-model. We demonstrate that this policy causes the agent to explore novel and informative interactions with its environment, leading to the generation of a spectrum of complex behaviors, including ego-motion prediction, object attention, and object gathering. Moreover, the world-model that the agent learns supports improved performance on object dynamics prediction, detection, localization and recognition tasks. Taken together, our results are initial steps toward creating flexible autonomous agents that self-supervise in realistic physical environments.},
 author = {Nick Haber and Damian Mrowca and Stephanie Wang and Li F Fei-Fei and Daniel L Yamins},

 journal = {Advances in Neural Information Processing Systems},
 year = {2018},
 title = {Learning to play with intrinsically-motivated, self-aware agents},
 selected={true},
 volume = {31}
}


@article{washington_superpowerglass_2017,
 abstract = {We have developed a system for automatic facial expression recognition running on Google Glass, delivering real-time social cues to children with Autism Spectrum Disorder (ASD). The system includes multiple mechanisms to engage children and their parents, who administer this technology within the home. We completed an at-home design trial with 14 families that used the learning aid over a 3-month period. We found that children with ASD generally respond well to wearing the system at home and opt for the most expressive feedback choice. We further evaluated app usage, facial engagement, and model accuracy. We found that the device can act as a powerful training aid when used periodically in the home, that interactive video content from wearable therapy sessions should be augmented with sufficient context about the content to produce long-term engagement, and that the design of wearable systems …},
 author = {Peter Washington and Catalin Voss and Aaron Kline and Nick Haber and Jena Daniels and Azar Fazel and Titas De and Carl Feinstein and Terry Winograd and Dennis Wall},

 journal = {Proceedings of the ACM on interactive, mobile, wearable and ubiquitous technologies},
 number = {3},
 pages = {1-22},
 year = {2017},
 publisher = {ACM},
 title = {SuperpowerGlass: a wearable aid for the at-home therapy of children with autism},
 volume = {1}
}


@article{washington_data-driven_2020,
 abstract = {Data science and digital technologies have the potential to transform diagnostic classification. Digital technologies enable the collection of big data, and advances in machine learning and artificial intelligence enable scalable, rapid, and automated classification of medical conditions. In this review, we summarize and categorize various data-driven methods for diagnostic classification. In particular, we focus on autism as an example of a challenging disorder due to its highly heterogeneous nature. We begin by describing the frontier of data science methods for the neuropsychiatry of autism. We discuss early signs of autism as defined by existing pen-and-paper–based diagnostic instruments and describe data-driven feature selection techniques for determining the behaviors that are most salient for distinguishing children with autism from neurologically typical children. We then describe data-driven detection …},
 author = {Peter Washington and Natalie Park and Parishkrita Srivastava and Catalin Voss and Aaron Kline and Maya Varma and Qandeel Tariq and Haik Kalantarian and Jessey Schwartz and Ritik Patnaik and Brianna Chrisman and Nathaniel Stockham and Kelley Paskov and Nick Haber and Dennis P Wall},

 number = {8},
 pages = {759-769},
 year = {2020},
 publisher = {Elsevier},
 title = {Data-driven diagnostics and the potential of mobile artificial intelligence for digital therapeutic phenotyping in computational psychiatry},
 volume = {5}
}


@article{duda_crowdsourced_2017,
 abstract = {Autism spectrum disorder (ASD) and attention deficit hyperactivity disorder (ADHD) together affect> 10% of the children in the United States, but considerable behavioral overlaps between the two disorders can often complicate differential diagnosis. Currently, there is no screening test designed to differentiate between the two disorders, and with waiting times from initial suspicion to diagnosis upwards of a year, methods to quickly and accurately assess risk for these and other developmental disorders are desperately needed. In a previous study, we found that four machine-learning algorithms were able to accurately (area under the curve (AUC)> 0.96) distinguish ASD from ADHD using only a small subset of items from the Social Responsiveness Scale (SRS). Here, we expand upon our prior work by including a novel crowdsourced data set of responses to our predefined top 15 SRS-derived questions from …},
 author = {M Duda and N Haber and J Daniels and DP Wall},

 journal = {Translational psychiatry},
 number = {5},
 pages = {e1133-e1133},
 year = {2017},
 publisher = {Nature Publishing Group},
 title = {Crowdsourced validation of a machine-learning classification system for autism and ADHD},
 volume = {7}
}


@article{levy_sparsifying_2017,
 abstract = {Autism spectrum disorder (ASD) diagnosis can be delayed due in part to the time required for administration of standard exams, such as the Autism Diagnostic Observation Schedule (ADOS). Shorter and potentially mobilized approaches would help to alleviate bottlenecks in the healthcare system. Previous work using machine learning suggested that a subset of the behaviors measured by ADOS can achieve clinically acceptable levels of accuracy. Here we expand on this initial work to build sparse models that have higher potential to generalize to the clinical population.We assembled a collection of score sheets for two ADOS modules, one for children with phrased speech (Module 2; 1319 ASD cases, 70 controls) and the other for children with verbal fluency (Module 3; 2870 ASD cases, 273 controls). We used sparsity/parsimony …},
 author = {Sebastien Levy and Marlena Duda and Nick Haber and Dennis P Wall},

 journal = {Molecular autism},
 pages = {1-17},
 year = {2017},
 publisher = {BioMed Central},
 title = {Sparsifying machine learning models identify stable subsets of predictive features for behavioral detection of autism},
 volume = {8}
}


@article{washington_a_2016,
 abstract = {Over 1 million children under the age of 17 in the US have been identified with Autism Spectrum Disorder (ASD). These children struggle to recognize facial expressions, make eye contact, and engage in social interactions. Gaining these skills requires intensive behavioral interventions that are often expensive, difficult to access, and inconsistently administered.nWe have developed a system to automate facial expression recognition that runs on wearable glasses and delivers real time social cues, with the goal of creating a behavioral aid for children with ASD that maximizes behavioral feedback while minimizing the distractions to the child. This paper describes the design of our system and interface decisions resulting from initial observations gathered during multiple preliminary trials.},
 author = {Peter Washington and Catalin Voss and Nick Haber and Serena Tanaka and Jena Daniels and Carl Feinstein and Terry Winograd and Dennis Wall},

 pages = {2348-2354},
 year = {2016},
 title = {A wearable social interaction aid for children with autism}
}


@article{wang_hypothesis_2023,
 abstract = {Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which can then be robustly generalized to novel scenarios. Recent work has evaluated large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding "in context learning." This can work well for straightforward inductive tasks, but performs very poorly on more complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be directly verified by running on the observed examples and generalized to novel inputs. Because of the prohibitive cost of generation with state-of-the-art LLMs, we consider a middle step to filter the set of hypotheses that will be implemented into programs: we either ask the LLM to summarize into a smaller set of hypotheses, or ask human annotators to select a subset of the hypotheses. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, and string transformation dataset SyGuS. On a random 40-problem subset of ARC, our automated pipeline using LLM summaries achieves 27.5% accuracy, significantly outperforming the direct prompting baseline (accuracy of 12.5%). With the minimal human input of selecting from LLM-generated candidates, the performance is boosted …},
 author = {Ruocheng Wang and Eric Zelikman and Gabriel Poesia and Yewen Pu and Nick Haber and Noah D Goodman},

 journal = {arXiv preprint arXiv:2309.05660},
 year = {2023},
 title = {Hypothesis search: Inductive reasoning with language models}
}


@article{voss_superpower_2016,
 abstract = {We have developed a system for automatic facial expression recognition, which runs on Google Glass and delivers real-time social cues to the wearer. We evaluate the system as a behavioral aid for children with Autism Spectrum Disorder (ASD), who can greatly benefit from real-time non-invasive emotional cues and are more sensitive to sensory input than neurotypically developing children. In addition, we present a mobile application that enables users of the wearable aid to review their videos along with auto-curated emotional information on the video playback bar. This integrates our learning aid into the context of behavioral therapy. Expanding on our previous work describing in-lab trials, this paper presents our system and application-level design decisions in depth as well as the interface learnings gathered during the use of the system by multiple children with ASD in an at-home iterative trial.},
 author = {Catalin Voss and Peter Washington and Nick Haber and Aaron Kline and Jena Daniels and Azar Fazel and Titas De and Beth McCarthy and Carl Feinstein and Terry Winograd and Dennis Wall},
 author = {C Voss and P Washington and N Haber},
 journal = {2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
 year = {2016},
 pages = {1218-1226},
 year = {2016},
 title = {Superpower glass: delivering unobtrusive real-time social cues in wearable systems}
}


@article{daniels_exploratory_2018,
 abstract = {Although standard behavioral interventions for autism spectrum disorder (ASD) are effective therapies for social deficits, they face criticism for being time-intensive and overdependent on specialists. Earlier starting age of therapy is a strong predictor of later success, but waitlists for therapies can be 18 months long. To address these complications, we developed Superpower Glass, a machine-learning-assisted software system that runs on Google Glass and an Android smartphone, designed for use during social interactions. This pilot exploratory study examines our prototype tool’s potential for social-affective learning for children with autism. We sent our tool home with 14 families and assessed changes from intake to conclusion through the Social Responsiveness Scale (SRS-2), a facial affect recognition task (EGG), and qualitative parent reports. A repeated-measures one-way ANOVA demonstrated a decrease in …},
 author = {Jena Daniels and Jessey N Schwartz and Catalin Voss and Nick Haber and Azar Fazel and Aaron Kline and Peter Washington and Carl Feinstein and Terry Winograd and Dennis P Wall},

 journal = {NPJ digital medicine},
 number = {1},
 pages = {32},
 year = {2018},
 publisher = {Nature Publishing Group UK},
 title = {Exploratory study examining the at-home feasibility of a wearable tool for social-affective learning in children with autism},
 volume = {1}
}


@article{daniels_feasibility_2018,
 abstract = { Background Recent advances in computer vision and wearable technology have created an opportunity to introduce mobile therapy systems for autism spectrum disorders (ASD) that can respond to the increasing demand for therapeutic interventions; however, feasibility questions must be answered first. Objective We studied the feasibility of a prototype therapeutic tool for children with ASD using Google Glass, examining whether children with ASD would wear such a device, if providing the emotion classification will improve emotion recognition, and how emotion recognition differs between ASD participants and neurotypical controls (NC). Methods We ran a controlled laboratory experiment with 43 children: 23 with ASD and 20 NC. Children identified static facial images on a computer screen with one of 7 emotions in 3 successive batches: the first with no information about emotion provided to the child, the …},
 author = {Jena Daniels and Nick Haber and Catalin Voss and Jessey Schwartz and Serena Tamura and Azar Fazel and Aaron Kline and Peter Washington and Jennifer Phillips and Terry Winograd and Carl Feinstein and Dennis P Wall},

 journal = {Applied clinical informatics},
 number = {01},
 pages = {129-140},
 year = {2018},
 publisher = {Schattauer GmbH},
 title = {Feasibility testing of a wearable behavioral aid for social learning in children with autism},
 volume = {9}
}


@article{zelikman_quiet-star:_2024,
 abstract = {When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought's start and end, and an extended teacher-forcing technique. Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM's ability to directly answer difficult questions. In particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero-shot improvements on GSM8K (5.9%10.9%) and CommonsenseQA (36.3%47.2%) and observe a perplexity improvement of …},
 author = {Eric Zelikman and Georges Harik and Yijia Shao and Varuna Jayasiri and Nick Haber and Noah D Goodman},

 journal = {arXiv preprint arXiv:2403.09629},
 year = {2024},
 title = {Quiet-star: Language models can teach themselves to think before speaking}
}


@article{zelikman_parsel_2023,
 abstract = {Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75\% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@ 1 performance on HumanEval from 67\% to 85\%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github. com/ezelikman/parsel.},
 author = {Eric Zelikman and Qian Huang and Gabriel Poesia and Noah Goodman and Nick Haber},

 journal = {Advances in Neural Information Processing Systems},
 pages = {31466-31523},
 year = {2023},
 title = {Parsel🐍: Algorithmic Reasoning with Language Models by Composing Decompositions},
 volume = {36},
 selected={true},
}


@article{kim_active_2020,
 abstract = {World models are self-supervised predictive models of how the world evolves. Humans learn world models by curiously exploring their environment, in the process acquiring compact abstractions of high bandwidth sensory inputs, the ability to plan across long temporal horizons, and an understanding of the behavioral patterns of other agents. In this work, we study how to design such a curiosity-driven Active World Model Learning (AWML) system. To do so, we construct a curious agent building world models while visually exploring a 3D physical environment rich with distillations of representative real-world agents. We propose an AWML system driven by -Progress: a scalable and effective learning progress-based curiosity signal and show that -Progress naturally gives rise to an exploration policy that directs attention to complex but learnable dynamics in a balanced manner, as a result overcoming the “white noise problem”. As a result, our -Progress-driven controller achieves significantly higher AWML performance than baseline controllers equipped with state-of-the-art exploration strategies such as Random Network Distillation and Model Disagreement.},
 author = {Kuno Kim and Megumi Sano and Julian De Freitas and Nick Haber and Daniel Yamins},

 conference = {International conference on machine learning},
 pages = {5306-5315},
 year = {2020},
 publisher = {PMLR},
 title = {Active world model learning with progress curiosity}
}


@article{kalantarian_guess_2019,
 abstract = {Autism Spectrum Disorder (ASD) is a condition affecting an estimated 1 in 59 children in the United States. Due to delays in diagnosis and imbalances in coverage, it is necessary to develop new methods of care delivery that can appropriately empower children and caregivers by capitalizing on mobile tools and wearable devices for use outside of clinical settings. In this paper, we present a mobile charades-style game, Guess What?, used for the acquisition of structured video from children with ASD for behavioral disease research. We then apply face tracking and emotion recognition algorithms to videos acquired through Guess What? game play. By analyzing facial affect in response to various prompts, we demonstrate that engagement and facial affect can be quantified and measured using real-time image processing algorithms: an important first-step for future therapies, at-home screenings, and …},
 author = {Haik Kalantarian and Peter Washington and Jessey Schwartz and Jena Daniels and Nick Haber and Dennis P Wall},

 journal = {Journal of healthcare informatics research},
 pages = {43-66},
 year = {2019},
 publisher = {Springer International Publishing},
 title = {Guess what? Towards understanding autism from structured video using facial affect},
 volume = {3}
}


@article{kalantarian_a_2018,
 abstract = {Autism Spectrum Disorder (ASD) is a condition affecting 70 million children worldwide. Due to delays in diagnosis and imbalances in coverage, it is necessary to develop new methods of care delivery for use outside of clinical settings., We present a mobile charades-style game, Guess What?, used for acquisition of structured video from children with ASD for behavioral disease research. By analyzing facial affect in response to various prompts, we demonstrate that engagement and facial affect can be quantified and measured using real-time image processing algorithms: an important step for future therapies, diagnostics, and outcome measures based on home video. Our study of thirteen subjects reveals that game sessions displaying "faces" to the player produced the most emotive facial expressions in the player by a considerable margin. Videos from the younger neurotypical group contained 73.9% more frames …},
 author = {Haik Kalantarian and Peter Washington and Jessey Schwartz and Jena Daniels and Nick Haber and Dennis Wall},

 conference = {2018 IEEE international conference on healthcare informatics (ICHI)},
 pages = {350-352},
 year = {2018},
 publisher = {IEEE},
 title = {A gamified mobile system for crowdsourcing video for autism research}
}


@article{nag_toward_2020,
 abstract = {Several studies have shown that facial attention differs in children with autism. Measuring eye gaze and emotion recognition in children with autism is challenging, as standard clinical assessments must be delivered in clinical settings by a trained clinician. Wearable technologies may be able to bring eye gaze and emotion recognition into natural social interactions and settings.This study aimed to test: (1) the feasibility of tracking gaze using wearable smart glasses during a facial expression recognition task and (2) the ability of these gaze-tracking data, together with facial expression recognition responses, to distinguish children with autism from neurotypical controls (NCs).We compared the eye gaze and emotion recognition patterns of 16 children with autism spectrum disorder (ASD) and 17 children without ASD via wearable smart glasses fitted with a custom eye tracker. Children identified static facial expressions of images presented on a computer screen along with nonsocial distractors while wearing Google Glass and the eye tracker. Faces were presented in three trials, during one of which children received feedback in the form of the correct classification. We employed hybrid human-labeling and computer vision–enabled methods for pupil tracking and world–gaze translation calibration. We analyzed the impact of gaze and emotion recognition features in a prediction task aiming to distinguish children with ASD from NC participants.Gaze and emotion recognition patterns enabled the training of a classifier that distinguished …},
 author = {Anish Nag and Nick Haber and Catalin Voss and Serena Tamura and Jena Daniels and Jeffrey Ma and Bryan Chiang and Shasta Ramachandran and Jessey Schwartz and Terry Winograd and Carl Feinstein and Dennis P Wall},

 journal = {Journal of medical Internet research},
 number = {4},
 pages = {e13810},
 year = {2020},
 publisher = {JMIR Publications},
 title = {Toward continuous social phenotyping: analyzing gaze patterns in an emotion recognition task for children with autism through wearable smart glasses},
 volume = {22}
}


@article{yang_holodeck:_2024,
 abstract = {3D simulated environments play a critical role in Embodied AI but their creation requires expertise and extensive manual effort restricting their diversity and scope. To mitigate this limitation we present Holodeck a system that generates 3D environments to match a user-supplied prompt fully automatedly. Holodeck can generate diverse scenes eg arcades spas and museums adjust the designs for styles and can capture the semantics of complex queries such as" apartment for a researcher with a cat" and" office of a professor who is a fan of Star Wars". Holodeck leverages a large language model (ie GPT-4) for common sense knowledge about what the scene might look like and uses a large collection of 3D assets from Objaverse to populate the scene with diverse objects. To address the challenge of positioning objects correctly we prompt GPT-4 to generate spatial relational constraints between objects and then optimize the layout to satisfy those constraints. Our large-scale human evaluation shows that annotators prefer Holodeck over manually designed procedural baselines in residential scenes and that Holodeck can produce high-quality outputs for diverse scene types. We also demonstrate an exciting application of Holodeck in Embodied AI training agents to navigate in novel scenes like music rooms and daycares without human-constructed data which is a significant step forward in developing general-purpose embodied agents.},
 author = {Yue Yang and Fan-Yun Sun and Luca Weihs and Eli VanderBilt and Alvaro Herrasti and Winson Han and Jiajun Wu and Nick Haber and Ranjay Krishna and Lingjie Liu and Chris Callison-Burch and Mark Yatskar and Aniruddha Kembhavi and Christopher Clark},

 conference = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 pages = {16227-16237},
 year = {2024},
 title = {Holodeck: Language guided generation of 3d embodied ai environments}
}


@article{washington_precision_2020,
 abstract = {Mobilized telemedicine is becoming a key, and even necessary, facet of both precision health and precision medicine. In this study, we evaluate the capability and potential of a crowd of virtual workers—defined as vetted members of popular crowdsourcing platforms—to aid in the task of diagnosing autism. We evaluate workers when crowdsourcing the task of providing categorical ordinal behavioral ratings to unstructured public YouTube videos of children with autism and neurotypical controls. To evaluate emerging patterns that are consistent across independent crowds, we target workers from distinct geographic loci on two crowdsourcing platforms: an international group of workers on Amazon Mechanical Turk (MTurk) (N = 15) and Microworkers from Bangladesh (N = 56), Kenya (N = 23), and the Philippines (N = 25). We feed worker responses as input to a validated diagnostic machine learning classifier trained on clinician-filled electronic health records. We find that regardless of crowd platform or targeted country, workers vary in the average confidence of the correct diagnosis predicted by the classifier. The best worker responses produce a mean probability of the correct class above 80% and over one standard deviation above 50%, accuracy and variability on par with experts according to prior studies. There is a weak correlation between mean time spent on task and mean performance (r = 0.358, p = 0.005). These results demonstrate that while the crowd can produce accurate diagnoses, there are intrinsic differences in crowdworker ability to rate behavioral features. We propose a novel strategy for recruitment of crowdsourced workers to …},
 author = {Peter Washington and Emilie Leblanc and Kaitlyn Dunlap and Yordan Penev and Aaron Kline and Kelley Paskov and Min Woo Sun and Brianna Chrisman and Nathaniel Stockham and Maya Varma and Catalin Voss and Nick Haber and Dennis P Wall},

 journal = {Journal of personalized medicine},
 number = {3},
 pages = {86},
 year = {2020},
 publisher = {MDPI},
 title = {Precision telemedicine through crowdsourced machine learning: testing variability of crowd workers for video-based autism feature recognition},
 volume = {10}
}


@article{washington_feature_2020,
 abstract = {Autism Spectrum Disorder (ASD) is a complex neuropsychiatric condition with a highly heterogeneous phenotype. Following the work of Duda et al., which uses a reduced feature set from the Social Responsiveness Scale, Second Edition (SRS) to distinguish ASD from ADHD, we performed item-level question selection on answers to the SRS to determine whether ASD can be distinguished from non-ASD using a similarly small subset of questions. To explore feature redundancies between the SRS questions, we performed filter, wrapper, and embedded feature selection analyses. To explore the linearity of the SRS-related ASD phenotype, we then compressed the 65-question SRS into low-dimension representations using PCA, t-SNE, and a denoising autoencoder. We measured the performance of a multi-layer perceptron (MLP) classifier with the top-ranking questions as input. Classification using only the top …},
 author = {Peter Washington and Kelley Marie Paskov and Haik Kalantarian and Nathaniel Stockham and Catalin Voss and Aaron Kline and Ritik Patnaik and Brianna Chrisman and Maya Varma and Qandeel Tariq and Kaitlyn Dunlap and Jessey Schwartz and Nick Haber and Dennis P Wall},

 journal = {Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing},
 pages = {707},
 year = {2020},
 title = {Feature selection and dimension reduction of social autism data},
 volume = {25}
}


@article{gell-redman_the_2016,
 abstract = {In this paper we analyze the Feynman wave equation on Lorentzian scattering spaces. We prove that the Feynman propagator exists as a map between certain Banach spaces defined by decay and microlocal Sobolev regularity properties. We go on to show that certain nonlinear wave equations arising in QFT are well-posed for small data in the Feynman setting.},
 author = {Jesse Gell-Redman and Nick Haber and András Vasy},

 journal = {Communications in Mathematical Physics},
 pages = {333-384},
 year = {2016},
 publisher = {Springer Berlin Heidelberg},
 title = {The Feynman propagator on perturbations of Minkowski space},
 volume = {342}
}


@article{washington_validity_2019,
 abstract = {Obtaining a diagnosis of neuropsychiatric disorders such as autism requires long waiting times that can exceed a year and can be prohibitively expensive. Crowdsourcing approaches may provide a scalable alternative that can accelerate general access to care and permit underserved populations to obtain an accurate diagnosis.We aimed to perform a series of studies to explore whether paid crowd workers on Amazon Mechanical Turk (AMT) and citizen crowd workers on a public website shared on social media can provide accurate online detection of autism, conducted via crowdsourced ratings of short home video clips.Three online studies were performed: (1) a paid crowdsourcing task on AMT (N=54) where crowd workers were asked to classify 10 short video clips of children as “Autism” or “Not autism,” (2) a more complex paid crowdsourcing task (N=27) with only those raters who correctly rated ≥8 of the 10 videos during the first study, and (3) a public unpaid study (N=115) identical to the first study.For Study 1, the mean score of the participants who completed all questions was 7.50/10 (SD 1.46). When only analyzing the workers who scored ≥8/10 (n=27/54), there was a weak negative correlation between the time spent rating the videos and the sensitivity (ρ=–0.44, P=.02). For Study 2, the mean score of the participants rating new videos was 6.76/10 (SD 0.59). The average deviation between the crowdsourced answers and gold standard ratings provided by two expert clinical research coordinators was 0.56, with an SD …},
 author = {Peter Washington and Haik Kalantarian and Qandeel Tariq and Jessey Schwartz and Kaitlyn Dunlap and Brianna Chrisman and Maya Varma and Michael Ning and Aaron Kline and Nathaniel Stockham and Kelley Paskov and Catalin Voss and Nick Haber and Dennis Paul Wall},

 journal = {Journal of medical Internet research},
 number = {5},
 pages = {e13668},
 year = {2019},
 publisher = {JMIR Publications},
 title = {Validity of online screening for autism: crowdsourcing study comparing paid and unpaid diagnostic tasks},
 volume = {21}
}


@article{kline_superpower_2019,
 abstract = {Autism Spectrum Disorder (ASD) is quickly becoming a global health crisis estimated to affect one in 40 children in the United States [8]. ASD consists of social deficiencies, such as impaired communication, eye contact, facial expression recognition, and social interaction. The current standard of care, applied behavioral analysis (ABA), relies on teaching these skills primarily in clinical environments with tools such as static flashcards. Such tools are largely removed from real world emotional contexts. While ABA can lead to improvements [1, 2], the therapy often generalizes poorly to situations that extend beyond the routines practiced in clinical contexts [3]. Furthermore, access to such treatment is constrained by the availability of therapists, who struggle to keep up with the increasing demand for care.},
 author = {Aaron Kline and Catalin Voss and Peter Washington and Nick Haber and Hessey Schwartz and Qandeel Tariq and Terry Winograd and Carl Feinstein and Dennis P Wall},

 journal = {GetMobile: Mobile Computing and Communications},
 number = {2},
 pages = {35-38},
 year = {2019},
 publisher = {ACM},
 title = {Superpower glass},
 volume = {23}
}


@article{wang_examining_2024,
 abstract = {The study explores the capabilities of OpenAI's ChatGPT in solving different types of physics problems. ChatGPT (with GPT-4) was queried to solve a total of 40 problems from a college-level engineering physics course. These problems ranged from well-specified problems, where all data required for solving the problem was provided, to under-specified, real-world problems where not all necessary data were given. Our findings show that ChatGPT could successfully solve 62.5% of the well-specified problems, but its accuracy drops to 8.3% for under-specified problems. Analysis of the model's incorrect solutions revealed three distinct failure modes: (1) failure to construct accurate models of the physical world, (2) failure to make reasonable assumptions about missing data, and (3) calculation errors. The study offers implications for how to leverage LLM-augmented instructional materials to enhance STEM education. The insights also contribute to the broader discourse on AI's strengths and limitations, serving both educators aiming to leverage the technology and researchers investigating human-AI collaboration frameworks for problem-solving and decision-making.},
 author = {Karen D Wang and Eric Burkholder and Carl Wieman and Shima Salehi and Nick Haber},

 journal = {Frontiers in Education},
 pages = {1330486},
 year = {2024},
 publisher = {Frontiers Media SA},
 title = {Examining the potential and pitfalls of ChatGPT in science and engineering problem-solving},
 volume = {8}
}


@article{washington_crowdsourced_2021,
 abstract = {Standard medical diagnosis of mental health conditions requires licensed experts who are increasingly outnumbered by those at risk, limiting reach. We test the hypothesis that a trustworthy crowd of non-experts can efficiently annotate behavioral features needed for accurate machine learning detection of the common childhood developmental disorder Autism Spectrum Disorder (ASD) for children under 8 years old. We implement a novel process for identifying and certifying a trustworthy distributed workforce for video feature extraction, selecting a workforce of 102 workers from a pool of 1,107. Two previously validated ASD logistic regression classifiers, evaluated against parent-reported diagnoses, were used to assess the accuracy of the trusted crowd’s ratings of unstructured home videos. A representative balanced sample (N = 50 videos) of videos were evaluated with and without face box and pitch shift …},
 author = {Peter Washington and Qandeel Tariq and Emilie Leblanc and Brianna Chrisman and Kaitlyn Dunlap and Aaron Kline and Haik Kalantarian and Yordan Penev and Kelley Paskov and Catalin Voss and Nathaniel Stockham and Maya Varma and Arman Husic and Jack Kent and Nick Haber and Terry Winograd and Dennis P Wall},

 journal = {Scientific reports},
 number = {1},
 pages = {7620},
 year = {2021},
 publisher = {Nature Publishing Group UK},
 title = {Crowdsourced privacy-preserved feature tagging of short home videos for machine learning ASD detection},
 volume = {11}
}


@article{haber_propagation_2012,
 abstract = {Para>This talk discusses the wavefront set of a solution u to Pu = f, where P is a pseudodifferential operator on a manifold with real-valued homogeneous principal symbol p, when the Hamilton vector field corresponding to p is radial on a Lagrangian submanifold contained in the characteristic set of P. According to a theorem of Duistermaat-Hörmander [2], singularities propagate along bicharacteristics of this Hamilton vector field. This theorem gives us no information about the wavefront set when the Hamilton vector field is radial.},
 author = {Nick Haber and András Vasy},

 pages = {113-116},
 year = {2012},
 publisher = {Springer Basel},
 title = {Propagation of singularities around a Lagrangian submanifold of radial points}
}


@article{washington_improved_2022,
 abstract = {Automated emotion classification could aid those who struggle to recognize emotions, including children with developmental behavioral conditions such as autism. However, most computer vision emotion recognition models are trained on adult emotion and therefore underperform when applied to child faces.We designed a strategy to gamify the collection and labeling of child emotion–enriched images to boost the performance of automatic child emotion recognition models to a level closer to what will be needed for digital health care approaches.We leveraged our prototype therapeutic smartphone game, GuessWhat, which was designed in large part for children with developmental and behavioral conditions, to gamify the secure collection of video data of children expressing a variety of emotions prompted by the game. Independently, we created a secure web interface to gamify the human labeling effort, called HollywoodSquares, tailored for use by any qualified labeler. We gathered and labeled 2155 videos, 39,968 emotion frames, and 106,001 labels on all images. With this drastically expanded pediatric emotion–centric database (>30 times larger than existing public pediatric emotion data sets), we trained a convolutional neural network (CNN) computer vision classifier of happy, sad, surprised, fearful, angry, disgust, and neutral expressions evoked by children.The classifier achieved a 66.9% balanced accuracy and 67.4% F1-score on the entirety of the Child Affective Facial Expression (CAFE) as well as a 79.1% balanced …},
 author = {Peter Washington and Haik Kalantarian and John Kent and Arman Husic and Aaron Kline and Emilie Leblanc and Cathy Hou and Onur Cezmi Mutlu and Kaitlyn Dunlap and Yordan Penev and Maya Varma and Nate Tyler Stockham and Brianna Chrisman and Kelley Paskov and Min Woo Sun and Jae-Yoon Jung and Catalin Voss and Nick Haber and Dennis Paul Wall},

 journal = {JMIR pediatrics and parenting},
 number = {2},
 pages = {e26760},
 year = {2022},
 publisher = {JMIR Publications},
 title = {Improved digital therapy for developmental pediatrics using domain-specific artificial intelligence: machine learning study},
 volume = {5}
}


@article{haber_making_2020,
 abstract = {Now imagine that Jimmy is wearing a special kind of Google Glass, the augmented-reality headset that Google introduced in 2013. When he looks up at his mom, the head-up display lights up with a green box, which alerts Jimmy that he's "found a face." As he focuses on her face, an emoji pops up, which tells Jimmy, "You found an angry face." He thinks about why his mom might be annoyed. Maybe he should stop what he's doing with the silverware and ask her.},
 author = {Nick Haber and Catalin Voss and Dennis Wall},

 journal = {IEEE Spectrum},
 number = {4},
 pages = {46-52},
 year = {2020},
 publisher = {IEEE},
 title = {Making emotions transparent: Google Glass helps autistic kids understand facial expressions through augmented-reaiity therapy},
 volume = {57}
}


@article{washington_selection_2021,
 abstract = {Crowd-powered telemedicine has the potential to revolutionize healthcare, especially during times that require remote access to care. However, sharing private health data with strangers from around the world is not compatible with data privacy standards, requiring a stringent filtration process to recruit reliable and trustworthy workers who can go through the proper training and security steps. The key challenge, then, is to identify capable, trustworthy, and reliable workers through high-fidelity evaluation tasks without exposing any sensitive patient data during the evaluation process. We contribute a set of experimentally validated metrics for assessing the trustworthiness and reliability of crowd workers tasked with providing behavioral feature tags to unstructured videos of children with autism and matched neurotypical controls. The workers are blinded to diagnosis and blinded to the goal of using the features to …},
 author = {Peter Washington and Emilie Leblanc and Kaitlyn Dunlap and Yordan Penev and Maya Varma and Jae-Yoon Jung and Brianna Chrisman and Min Woo Sun and Nathaniel Stockham and Kelley Marie Paskov and Haik Kalantarian and Catalin Voss and Nick Haber and Dennis P Wall},

 journal = {Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing},
 pages = {14},
 year = {2021},
 title = {Selection of trustworthy crowd workers for telemedical diagnosis of pediatric autism spectrum disorder},
 volume = {26}
}


@article{haber_a_2016,
 abstract = {Methods for automated facial expression recognition - identifying faces as happy, sad, angry, etc. - typically rely on the classification of features extracted from images. These features, designed to encode shape and texture information, depend on both (1) the expression an individual is making, and (2) the individual's physical characteristics and lighting conditions of the image. To reduce the effect of (2), a common strategy is to establish a "baseline" for an individual and subtract out this individual's baseline neutral feature. This extra neutral feature information often is not available - in particular for in-the-wild, real-time classification of a previously unseen subject. Thus, in order to implement "neutral subtraction," one must estimate the individual's neutral feature. Existing methods to do this are susceptible to class imbalance at test time (e.g., averaging over all facial features), require a more complex model specific to …},
 author = {Nick Haber and Catalin Voss and Azar Fazel and Terry Winograd and Dennis P Wall},

 conference = {2016 IEEE Winter Conference on Applications of Computer Vision (WACV)},
 pages = {1-9},
 year = {2016},
 publisher = {IEEE},
 title = {A practical approach to real-time neutral feature subtraction for facial expression recognition}
}


@article{haber_emergence_2018,
 abstract = {Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to replicate some of these abilities with a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulated environment in which the agent can move and interact with objects it sees, the agent learns a world model predicting the dynamic consequences of its actions. Simultaneously, the agent learns to take actions that adversarially challenge the developing world model, pushing the agent to explore novel and informative interactions with its environment. We demonstrate that this policy leads to the self-supervised emergence of a spectrum of complex behaviors, including ego motion prediction, object attention, and object gathering. Moreover, the world model that the agent learns supports improved performance on object dynamics prediction and localization tasks. Our results are a proof-of-principle that computational models of intrinsic motivation might account for key features of developmental visuomotor learning in infants.},
 author = {Nick Haber and Damian Mrowca and Li Fei-Fei and Daniel LK Yamins},

 journal = {arXiv preprint arXiv:1802.07461},
 year = {2018},
 title = {Emergence of structured behaviors from curiosity-based intrinsic motivation}
}


@article{washington_training_2020,
 abstract = {Automated emotion classification could aid those who struggle to recognize emotion, including children with developmental behavioral conditions such as autism. However, most computer vision emotion models are trained on adult affect and therefore underperform on child faces. In this study, we designed a strategy to gamify the collection and the labeling of child affect data in an effort to boost the performance of automatic child emotion detection to a level closer to what will be needed for translational digital healthcare. We leveraged our therapeutic smartphone game, GuessWhat, which was designed in large part for children with developmental and behavioral conditions, to gamify the secure collection of video data of children expressing a variety of emotions prompted by the game. Through a secure web interface gamifying the human labeling effort, we gathered and labeled 2,155 videos, 39,968 emotion frames, and 106,001 labels on all images. With this drastically expanded pediatric emotion centric database (>30x larger than existing public pediatric affect datasets), we trained a pediatric emotion classification convolutional neural network (CNN) classifier of happy, sad, surprised, fearful, angry, disgust, and neutral expressions in children. The classifier achieved 66.9% balanced accuracy and 67.4% F1-score on the entirety of CAFE as well as 79.1% balanced accuracy and 78.0% F1-score on CAFE Subset A, a subset containing at least 60% human agreement on emotions labels. This performance is at least 10% higher than all previously published classifiers, the best of which reached 56.% balanced accuracy even when combining …},
 author = {Peter Washington and Haik Kalantarian and Jack Kent and Arman Husic and Aaron Kline and Emilie Leblanc and Cathy Hou and Cezmi Mutlu and Kaitlyn Dunlap and Yordan Penev and Maya Varma and Nate Stockham and Brianna Chrisman and Kelley Paskov and Min Woo Sun and Jae-Yoon Jung and Catalin Voss and Nick Haber and Dennis P Wall},

 journal = {arXiv preprint arXiv:2012.08678},
 year = {2020},
 title = {Training an emotion detection classifier using frames from a mobile therapeutic game for children with developmental disorders}
}


@article{sun_interaction_2022,
 abstract = {Modeling multi-agent systems requires understanding how agents interact. Such systems are often difficult to model because they can involve a variety of types of interactions that layer together to drive rich social behavioral dynamics. Here we introduce a method for accurately modeling multi-agent systems. We present Interaction Modeling with Multiplex Attention (IMMA), a forward prediction model that uses a multiplex latent graph to represent multiple independent types of interactions and attention to account for relations of different strengths. We also introduce Progressive Layer Training, a training strategy for this architecture. We show that our approach outperforms state-of-the-art models in trajectory forecasting and relation inference, spanning three multi-agent scenarios: social navigation, cooperative task achievement, and team sports. We further demonstrate that our approach can improve zero-shot generalization and allows us to probe how different interactions impact agent behavior.},
 author = {Fan-Yun Sun and Isaac Kauvar and Ruohan Zhang and Jiachen Li and Mykel J Kochenderfer and Jiajun Wu and Nick Haber},

 journal = {Advances in Neural Information Processing Systems},
 pages = {20038-20050},
 year = {2022},
 title = {Interaction modeling with multiplex attention},
 volume = {35}
}


@article{washington_training_2021,
 abstract = {Emotion detection classifiers traditionally predict discrete emotions. However, emotion expressions are often subjective, thus requiring a method to handle compound and ambiguous labels. We explore the feasibility of using crowdsourcing to acquire reliable soft-target labels and evaluate an emotion detection classifier trained with these labels. We hypothesize that training with labels that are representative of the diversity of human interpretation of an image will result in predictions that are similarly representative on a disjoint test set. We also hypothesize that crowdsourcing can generate distributions which mirror those generated in a lab setting. We center our study on the Child Affective Facial Expression (CAFE) dataset, a gold standard collection of images depicting pediatric facial expressions along with 100 human labels per image. To test the feasibility of crowdsourcing to generate these labels, we …},
 author = {Peter Washington and Haik Kalantarian and Jack Kent and Arman Husic and Aaron Kline and Emilie Leblanc and Cathy Hou and Cezmi Mutlu and Kaitlyn Dunlap and Yordan Penev and Nate Stockham and Brianna Chrisman and Kelley Paskov and Jae-Yoon Jung and Catalin Voss and Nick Haber and Dennis P Wall},

 journal = {Cognitive computation},
 pages = {1363-1373},
 year = {2021},
 publisher = {Springer US},
 title = {Training affective computer vision models by crowdsourcing soft-target labels},
 volume = {13}
}


@article{hess_communication_2022,
 abstract = {Augmented reality (AR) has promise as a clinical teaching tool, particularly for remote learning. The Chariot Augmented Reality Medical (CHARM) simulator integrates real-time communication into a portable medical simulator with a holographic patient and monitor. The primary aim was to analyze feedback from medical and physician assistant students regarding acceptability and feasibility of the simulator.Using the CHARM simulator, we created an advanced cardiovascular life support (ACLS) simulation scenario. After IRB approval, preclinical medical and physician assistant students volunteered to participate from August to September 2020. We delivered augmented reality headsets (Magic Leap One) to students before the study. Prior to the simulation, via video conference, we introduced students to effective communication skills during a cardiac arrest. Participants then, individually and …},
 author = {Olivia Hess and Jimmy Qian and Janine Bruce and Ellen Wang and Samuel Rodriguez and Nick Haber and Thomas J Caruso},

 journal = {Medical science educator},
 number = {5},
 pages = {1005-1014},
 year = {2022},
 publisher = {Springer US},
 title = {Communication skills training using remote augmented reality medical simulation: a feasibility and acceptability qualitative study},
 volume = {32}
}


@article{haber_propagation_2015,
 abstract = {In this work we study the wavefront set of a solution u to Pu= f, where P is a pseudodifferential operator on a manifold with real-valued homogeneous principal symbol p, when the Hamilton vector field corresponding to p is radial on a Lagrangian submanifold Λ contained in the characteristic set of P. The standard propagation of singularities theorem of Duistermaat-Hörmander gives no information at Λ. By adapting the standard positive-commutator estimate proof of this theorem, we are able to conclude additional regularity at a point q in this radial set, assuming some regularity around this point. That is, the a priori assumption is either a weaker regularity assumption at q, or a regularity assumption near but not at q. Earlier results of Melrose and Vasy give a more global version of such analysis. Given some regularity assumptions around the Lagrangian submanifold, they obtain some regularity at the Lagrangian …},
 author = {Nick Haber and András Vasy},

 journal = {Bull. Soc. Math. France},
 number = {4},
 pages = {679-726},
 year = {2015},
 title = {Propagation of singularities around a Lagrangian submanifold of radial points},
 volume = {143}
}


@article{agg_5.13_2017,
 abstract = {Objectives: We investigated the feasibility of using physiological biomarkers to predict the onset of aggression in minimally verbal (MV) youth with autism spectrum disorder (ASD).Methods: Nine MV youth with confirmed ASD wore a wristband-mounted E4 biosensor during repeated unstructured observation periods while they were hospitalized in a specialized child psychiatry unit. Physiological and three axis acceleration data were collected concurrent with coding of aggressive behavior. Physiological arousal was measured by: 1) heart rate and heart rate variability, both derived from blood volume pulse (BVP) and interbeat interval (IBI) via photoplethysmography at 64 Hz; and 2) electrodermal activity (EDA), which reflects autonomic innervation of sweat glands. Advanced signal processing and machine learning algorithms were then applied to predict aggression onset. The area under the curve (AUC) accuracy (based on true/false positive rates) was calculated to predict the onset of aggression in the next one minute from present time (t). The predictions were made through a ridge-regularized logistic regression using: 1) previous t ¼ 3 minutes of motor movement acceleration (ACC) signals; 2) time elapsed since last aggression event; 3) previous t ¼ 3 minutes of BVP, EDA, and IBI signals; and 4) all of the above signals combined.Results: All youth tolerated the sensor after desensitization, usable data were obtained in all cases, and there was an average of 9.67 (range ¼ 0-44) aggressive episodes per four-hour observation period. Time-synced coding of aggression and concurrent E4 signal data predicted the onset of aggression with AUC …},
 author = {ASD AGG},

 journal = {Journal of the American Academy of Child & Adolescent Psychiatry},
 number = {10S},
 year = {2017},
 title = {5.13 Design and efficacy of a wearable device for social affective learning in children with autism},
 volume = {56}
}


@article{kauvar_curious_2023,
 abstract = {Agents must be able to adapt quickly as an environment changes. We find that existing model-based reinforcement learning agents are unable to do this well, in part because of how they use past experiences to train their world model. Here, we present Curious Replay -- a form of prioritized experience replay tailored to model-based agents through use of a curiosity-based priority signal. Agents using Curious Replay exhibit improved performance in an exploration paradigm inspired by animal behavior and on the Crafter benchmark. DreamerV3 with Curious Replay surpasses state-of-the-art performance on Crafter, achieving a mean score of 19.4 that substantially improves on the previous high score of 14.5 by DreamerV3 with uniform replay, while also maintaining similar performance on the Deepmind Control Suite. Code for Curious Replay is available at https://github.com/AutonomousAgentsLab/curiousreplay},
 author = {Isaac Kauvar and Chris Doyle and Linqi Zhou and Nick Haber},

 journal = {arXiv preprint arXiv:2306.15934},
 year = {2023},
 title = {Curious replay for model-based adaptation}
}


@article{voss_the_2019,
 abstract = {In Reply We agree with the commentary by Ahuja on our article. 1 Adolescents and adults need innovative digital solutions that can scale to meet what is an enormous and enormously diverse demand. Complex social interactions, such as dating, interviewing, managing conflict, and even just basic daily living skills, challenge this teenager-to-adult population in myriad ways. Naturally, this makes the focus on tools development significantly more challenging. What are the biggest unmet needs where the combination of artificial intelligence and augmented reality wearables (or similar tools) can make a positive effect? We can generate a list, and probably come up with a fairly exhaustive one, but how to prioritize that list is daunting.Our focus has been on the pediatric population namely because addressing developmental issues early can pay dividends downstream; children may progress to a point where they no …},
 author = {Catalin Voss and Nick Haber and Dennis P Wall},

 journal = {JAMA pediatrics},
 number = {11},
 pages = {1106-1106},
 year = {2019},
 publisher = {American Medical Association},
 title = {The potential for machine learning–based wearables to improve socialization in teenagers and adults with autism spectrum disorder—reply},
 volume = {173}
}


@article{washington_challenges_2022,
 abstract = {Computer Vision (CV) classifiers which distinguish and detect nonverbal social human behavior and mental state can aid digital diagnostics and therapeutics for psychiatry and the behavioral sciences. While CV classifiers for traditional and structured classification tasks can be developed with standard machine learning pipelines for supervised learning consisting of data labeling, preprocessing, and training a convolutional neural network, there are several pain points which arise when attempting this process for behavioral phenotyping. Here, we discuss the challenges and corresponding opportunities in this space, including handling heterogeneous data, avoiding biased models, labeling massive and repetitive data sets, working with ambiguous or compound class labels, managing privacy concerns, creating appropriate representations, and personalizing models. We discuss current state-of-the-art research endeavors in CV such as data curation, data augmentation, crowdsourced labeling, active learning, reinforcement learning, generative models, representation learning, federated learning, and meta-learning. We highlight at least some of the machine learning advancements needed for imaging classifiers to detect human social cues successfully and reliably.},
 author = {Peter Washington and Cezmi Onur Mutlu and Aaron Kline and Kelley Paskov and Nate Tyler Stockham and Brianna Chrisman and Nick Deveau and Mourya Surhabi and Nick Haber and Dennis P Wall},

 year = {2022},
 title = {Challenges and opportunities for machine learning classification of behavior and mental state from images}
}


@article{voss_titas_2016,
 author = {Catalin Voss and Peter Washington and Nick Haber and Aaron Kline and Jena Daniels and Azar Fazel},

 journal = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct (Heidelberg, Germany)(UbiComp’16). Association for Computing Machinery, New York, NY, USA},
 pages = {1218-1226},
 year = {2016},
 title = {Superpower glass: delivering unobtrusive real-time social cues in wearable systems}
}


@article{poesia_learning_2024,
 abstract = {How did humanity coax mathematics from the aether? We explore the Platonic view that mathematics can be discovered from its axioms - a game of conjecture and proof. We describe Minimo (Mathematics from Intrinsic Motivation): an agent that jointly learns to pose challenging problems for itself (conjecturing) and solve them (theorem proving). Given a mathematical domain axiomatized in dependent type theory, we first combine methods for constrained decoding and type-directed synthesis to sample valid conjectures from a language model. Our method guarantees well-formed conjectures by construction, even as we start with a randomly initialized model. We use the same model to represent a policy and value function for guiding proof search. Our agent targets generating hard but provable conjectures - a moving target, since its own theorem proving ability also improves as it trains. We propose novel methods for hindsight relabeling on proof search trees to significantly improve the agent's sample efficiency in both tasks. Experiments on 3 axiomatic domains (propositional logic, arithmetic and group theory) demonstrate that our agent can bootstrap from only the axioms, self-improving in generating true and challenging conjectures and in finding proofs.},
 author = {Gabriel Poesia and David Broman and Nick Haber and Noah D Goodman},

 journal = {arXiv preprint arXiv:2407.00695},
 year = {2024},
 title = {Learning formal mathematics from intrinsic motivation}
}


@article{zelikman_just_2023,
 abstract = {Language model training in distributed settings is limited by the communication cost of gradient exchanges. In this short note, we extend recent work from Malladi et al. (2023), using shared randomness to perform distributed fine-tuning with low bandwidth. The method is a natural decentralized extension of memory-efficient Simultaneous Perturbation Stochastic Approximation (SPSA). Each iteration, each machine seeds a Random Number Generator (RNG) to perform local reproducible perturbations on model weights and calculate and exchange scalar projected gradients, which are then used to update each model. By using a (machine, sample) identifier as the random seed, each model can regenerate one another's perturbations. As machines only exchange single-byte projected gradients, this is highly communication efficient. There are also potential privacy benefits, as projected gradients may be calculated on different training data, and models never access the other's data. Our approach not only drastically reduces communication bandwidth requirements but also accommodates dynamic addition or removal of machines during the training process and retains the memory-efficient and inference-only advantages of recent work. We perform proof-of-concept experiments to demonstrate the potential usefulness of this method, building off of rich literature on distributed optimization and memory-efficient training.},
 author = {Eric Zelikman and Qian Huang and Percy Liang and Nick Haber and Noah D Goodman},

 journal = {arXiv preprint arXiv:2306.10015},
 year = {2023},
 title = {Just one byte (per gradient): A note on low-bandwidth decentralized language model finetuning using shared randomness}
}


@article{caruso_integrated_2021,
 abstract = {Augmented reality (AR) has been studied as a clinical teaching tool, however eye-tracking capabilities integrated within an AR medical simulator have limited research. The recently developed Chariot Augmented Reality Medical (CHARM) simulator integrates real-time communication into a portable medical simulator. The purpose of this project was to refine the gaze-tracking capabilities of the CHARM simulator on the Magic Leap One (ML1). Adults aged 18 years and older were recruited using convenience sampling. Participants were provided with an ML1 headset that projected a hologram of a patient, bed and monitor. They were instructed via audio recording to gaze at variables in this scenario. The participant gaze targets from the ML1 output were compared with the specified gaze points from the audio recording. A priori investigators planned to iterative modifications of the eye-tracking software until a …},
 author = {Thomas J Caruso and Olivia Hess and Kenny Roy and Ellen Wang and Samuel Rodriguez and Coby Palivathukal and Nick Haber},

 journal = {BMJ simulation & technology enhanced learning},
 number = {5},
 pages = {431},
 year = {2021},
 publisher = {BMJ Publishing Group},
 title = {Integrated eye tracking on Magic Leap One during augmented reality medical simulation: a technical report},
 volume = {7}
}


@article{voss_designing_2020,
 abstract = {In recent years, much focus has been put on employing technology to make novel behavioural aids for those with autism. Most of these are digital adaptations of tools used in standard behavioural therapy to enforce normative skills. These digital counterparts are often used outside of both the larger therapeutic context and the real world, in which the learned skills might apply. To address this, we are designing a system of automatic expression recognition on wearable devices that integrates directly into the families daily social interactions, to give children and their caregivers the tools and information they need to design their own holistic therapy. In order to develop a tool that will be truly useful to families, we proactively include children with autism and their families as co-designers in the development process. By providing an app and interface with interchangeable social feedback options, we aim to produce a framework for therapy that folds into their daily lives, tailored to their specific needs.},
 author = {Catalin Voss and Nick Haber and Peter Washington and Aaron Kline and Beth McCarthy and Jena Daniels and Azar Fazel and Titas De and Carl Feinstein and Terry Winograd and Dennis Wall},

 journal = {arXiv preprint arXiv:2002.04263},
 year = {2020},
 title = {Designing a holistic at-home learning aid for autism}
}


@article{zelikman_generating_2023,
 abstract = {Developing an educational test can be expensive and time-consuming, as each item must be written by experts and then evaluated by collecting hundreds of student responses. Moreover, many tests require multiple distinct sets of questions administered throughout the school year to closely monitor students' progress, known as parallel tests. In this study, we focus on tests of silent sentence reading efficiency, used to assess students' reading ability over time. To generate high-quality parallel tests, we propose to fine-tune large language models (LLMs) to simulate how previous students would have responded to unseen items. With these simulated responses, we can estimate each item's difficulty and ambiguity. We first use GPT-4 to generate new test items following a list of expert-developed rules and then apply a fine-tuned LLM to filter the items based on criteria from psychological measurements. We also propose an optimal-transport-inspired technique for generating parallel tests and show the generated tests closely correspond to the original test's difficulty and reliability based on crowdworker responses. Our evaluation of a generated test with 234 students from grades 2 to 8 produces test scores highly correlated (r=0.93) to those of a standard test form written by human experts and evaluated across thousands of K-12 students.},
 author = {Eric Zelikman and Wanjing Anya Ma and Jasmine E Tran and Diyi Yang and Jason D Yeatman and Nick Haber},

 journal = {arXiv preprint arXiv:2310.06837},
 year = {2023},
 title = {Generating and evaluating tests for k-12 students with language model simulations: A case study on sentence reading efficiency}
}


@article{washington_crowdsourced_2020,
 abstract = {Standard medical diagnosis of mental health conditions often requires licensed experts who are increasingly outnumbered by those at risk, limiting reach. We test the hypothesis that a trustworthy crowd of non-experts can efficiently label features needed for accurate machine learning detection of the common childhood developmental disorder autism. We implement a novel process for creating a trustworthy distributed workforce for video feature extraction, selecting a workforce of 102 workers from a pool of 1,107. Two previously validated binary autism logistic regression classifiers were used to evaluate the quality of the curated crowd’s ratings on unstructured home videos. A clinically representative balanced sample (N=50 videos) of videos were evaluated with and without face box and pitch shift privacy alterations, with AUROC and AUPRC scores >0.98. With both privacy-preserving modifications, sensitivity is preserved (96.0%) while maintaining specificity (80.0%) and accuracy (88.0%) at levels that exceed classification methods without alterations. We find that machine learning classification from features extracted by a curated nonexpert crowd achieves clinical performance for pediatric autism videos and maintains acceptable performance when privacy-preserving mechanisms are applied. These results suggest that privacy-based crowdsourcing of short videos can be leveraged for rapid and mobile assessment of behavioral health.},
 author = {Peter Washington and Qandeel Tariq and Emilie Leblanc and Brianna Chrisman and Kaitlyn Dunlap and Aaron Kline and Haik Kalantarian and Yordan Penev and Kelley Paskov and Catalin Voss and Nathaniel Stockham and Maya Varma and Arman Husic and Jack Kent and Nick Haber and Terry Winograd and Dennis P Wall},

 journal = {medRxiv},
 pages = {2020.12. 15.20248283},
 year = {2020},
 publisher = {Cold Spring Harbor Laboratory Press},
 title = {Crowdsourced feature tagging for scalable and privacy-preserved autism diagnosis}
}


@article{haber_a_2020,
 abstract = {With most recent estimates giving an incidence rate of 1 in 68 children in the United States, the autism spectrum disorder (ASD) is a growing public health crisis. Many of these children struggle to make eye contact, recognize facial expressions, and engage in social interactions. Today the standard for treatment of the core autism-related deficits focuses on a form of behavior training known as Applied Behavioral Analysis. To address perceived deficits in expression recognition, ABA approaches routinely involve the use of prompts such as flash cards for repetitive emotion recognition training via memorization. These techniques must be administered by trained practitioners and often at clinical centers that are far outnumbered by and out of reach from the many children and families in need of attention. Waitlists for access are up to 18 months long, and this wait may lead to children regressing down a path of isolation that worsens their long-term prognosis. There is an urgent need to innovate new methods of care delivery that can appropriately empower caregivers of children at risk or with a diagnosis of autism, and that capitalize on mobile tools and wearable devices for use outside of clinical settings.},
 author = {Nick Haber and Catalin Voss and Jena Daniels and Peter Washington and Azar Fazel and Aaron Kline and Titas De and Terry Winograd and Carl Feinstein and Dennis P Wall},

 journal = {arXiv preprint arXiv:2004.14281},
 year = {2020},
 title = {A wearable social interaction aid for children with autism}
}


@article{washington_using_2021,
 author = {Peter Washington and Onur Cezmi Mutlu and Emilie Leblanc and Aaron Kline and Cathy Hou and Brianna Chrisman and Nate Stockham and Kelley Paskov and Catalin Voss and Nick Haber and Dennis P Wall},

 journal = {arXiv preprint arXiv:2101.03477},
 year = {2021},
 title = {Using crowdsourcing to train facial emotion machine learning models with ambiguous labels}
}


@article{lingelbach_towards_2020,
 abstract = {Throughout our lives, we as humans acquire an intuitive understanding of our physical environments, a capacity that supports our imagination and planning abilities. Driven by our own curiosity, we learn about object motion and properties via self-curated targeted experiments, that teach us what we do not know. Recently, neural network models have been proposed that learn forward object dynamics from observations like humans. Unlike humans, these models do not actively interact with surrounding objects but learn from human-curated datasets as passive observers. In this work-in-progress, we propose a closed-loop system that teaches itself about forward object dynamics without any human intervention. Our model consists of two parts. A forward dynamics model that models the transition between states and a policy model that tries to predict the dynamics model’s error conditioned on object interactions as its intrinsic reward. We show that our method is able to train forward dynamics models that generalize to unseen physical scenarios and approaches the upper bound of models trained on human-curated data. The model generates complex behaviors with a preference to novel objects.},
 author = {Michael Lingelbach and Damian Mrowca and Nick Haber and Li Fei-Fei and Daniel LK Yamins},

 journal = {ICLR Bridging AI and Cognitive Science (BAICS) Workshop},
 year = {2020},
 title = {Towards curiosity-driven learning of physical dynamics}
}


@article{ram_binding_2024,
 abstract = {Advances in ability to comprehensively record individuals’ digital lives and in AI modeling of those data facilitate new possibilities for describing, predicting, and generating a wide variety of behavioral processes. In this paper, we consider these advances from a person-specific perspective, including whether the pervasive concerns about generalizability of results might be productively reframed with respect to transferability of models, and how self-supervision and new deep neural network architectures that facilitate transfer learning can be applied in a person-specific way to the super-intensive longitudinal data arriving in the Human Screenome Project. In developing the possibilities, we suggest Molenaar add a statement to the person-specific Manifesto – “In short, the concerns about generalizability commonly leveled at the person-specific paradigm are unfounded and can be fully and completely replaced with …},
 author = {Nilam Ram and Nick Haber and Thomas N Robinson and Byron Reeves},

 journal = {Multivariate Behavioral Research},
 number = {6},
 pages = {1211-1219},
 year = {2024},
 publisher = {Routledge},
 title = {Binding the person-specific approach to modern AI in the human screenome project: moving past generalizability to transferability},
 volume = {59}
}


@article{cross_hypothetical_2024,
 abstract = {Multi-agent reinforcement learning (MARL) methods struggle with the non-stationarity of multi-agent systems and fail to adaptively learn online when tested with novel agents. Here, we leverage large language models (LLMs) to create an autonomous agent that can handle these challenges. Our agent, Hypothetical Minds, consists of a cognitively-inspired architecture, featuring modular components for perception, memory, and hierarchical planning over two levels of abstraction. We introduce the Theory of Mind module that scaffolds the high-level planning process by generating hypotheses about other agents' strategies in natural language. It then evaluates and iteratively refines these hypotheses by reinforcing hypotheses that make correct predictions about the other agents' behavior. Hypothetical Minds significantly improves performance over previous LLM-agent and RL baselines on a range of competitive, mixed motive, and collaborative domains in the Melting Pot benchmark, including both dyadic and population-based environments. Additionally, comparisons against LLM-agent baselines and ablations reveal the importance of hypothesis evaluation and refinement for succeeding on complex scenarios.},
 author = {Logan Cross and Violet Xiang and Agam Bhatia and Daniel LK Yamins and Nick Haber},

 journal = {arXiv preprint arXiv:2407.07086},
 year = {2024},
 title = {Hypothetical minds: Scaffolding theory of mind for multi-agent tasks with large language models}
}


@article{haber_a_2014,
 abstract = {In this work, we produce microlocal normal forms for pseudodifferential operators which have a Lagrangian submanifold of radial points. This answers natural questions about such operators and their associated classical dynamics. In a sequel, we will give a microlocal parametrix construction, as well as a construction of a microlocal Poisson operator, for such pseudodifferential operators.},
 author = {Nick Haber},

 journal = {International Mathematics Research Notices},
 number = {17},
 pages = {4804-4821},
 year = {2014},
 publisher = {OUP},
 title = {A normal form around a Lagrangian submanifold of radial points},
 volume = {2014}
}


@article{niknazar_building_2024,
 abstract = {Generative AI holds the promise of enabling a range of sought-after capabilities and revolutionizing workflows in various consumer and enterprise verticals. However, putting a model in production involves much more than just generating an output. It involves ensuring the model is reliable, safe, performant and also adheres to the policy of operation in a particular domain. Guardrails as a necessity for models has evolved around the need to enforce appropriate behavior of models, especially when they are in production. In this paper, we use education as a use case, given its stringent requirements of the appropriateness of content in the domain, to demonstrate how a guardrail model can be trained and deployed in production. Specifically, we describe our experience in building a production-grade guardrail model for a K-12 educational platform. We begin by formulating the requirements for deployment to this sensitive domain. We then describe the training and benchmarking of our domain-specific guardrail model, which outperforms competing open- and closed- instruction-tuned models of similar and larger size, on proprietary education-related benchmarks and public benchmarks related to general aspects of safety. Finally, we detail the choices we made on architecture and the optimizations for deploying this service in production; these range across the stack from the hardware infrastructure to the serving layer to language model inference optimizations. We hope this paper will be instructive to other practitioners looking to create production-grade domain-specific services based on generative AI and large language models.},
 author = {Mohammad Niknazar and Paul V Haley and Latha Ramanan and Sang T Truong and Yedendra Shrinivasan and Ayan Kumar Bhowmick and Prasenjit Dey and Ashish Jagmohan and Hema Maheshwari and Shom Ponoth and Robert Smith and Aditya Vempaty and Nick Haber and Sanmi Koyejo and Sharad Sundararajan},

 journal = {arXiv preprint arXiv:2408.01452},
 year = {2024},
 title = {Building a domain-specific guardrail model in production}
}


@article{kreiss_contextref:_2023,
 abstract = {Referenceless metrics (e.g., CLIPScore) use pretrained vision--language models to assess image descriptions directly without costly ground-truth reference texts. Such methods can facilitate rapid progress, but only if they truly align with human preference judgments. In this paper, we introduce ContextRef, a benchmark for assessing referenceless metrics for such alignment. ContextRef has two components: human ratings along a variety of established quality dimensions, and ten diverse robustness checks designed to uncover fundamental weaknesses. A crucial aspect of ContextRef is that images and descriptions are presented in context, reflecting prior work showing that context is important for description quality. Using ContextRef, we assess a variety of pretrained models, scoring functions, and techniques for incorporating context. None of the methods is successful with ContextRef, but we show that careful fine-tuning yields substantial improvements. ContextRef remains a challenging benchmark though, in large part due to the challenge of context dependence.},
 author = {Elisa Kreiss and Eric Zelikman and Christopher Potts and Nick Haber},

 journal = {arXiv preprint arXiv:2309.11710},
 year = {2023},
 title = {ContextRef: Evaluating Referenceless Metrics For Image Description Generation}
}


@article{xiang_towards_2025,
 abstract = {We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends traditional Chain-of-Thought (CoT) by explicitly modeling the underlying reasoning required to arrive at a particular CoT. We present empirical evidence from state-of-the-art models exhibiting behaviors consistent with in-context search, and explore methods for producing Meta-CoT via process supervision, synthetic data generation, and search algorithms. Finally, we outline a concrete pipeline for training a model to produce Meta-CoTs, incorporating instruction tuning with linearized search traces and reinforcement learning post-training. Finally, we discuss open research questions, including scaling laws, verifier roles, and the potential for discovering novel reasoning algorithms. This work provides a theoretical and practical roadmap to enable Meta-CoT in LLMs, paving the way for more powerful and human-like reasoning in artificial intelligence.},
 author = {Violet Xiang and Charlie Snell and Kanishk Gandhi and Alon Albalak and Anikait Singh and Chase Blagden and Duy Phung and Rafael Rafailov and Nathan Lile and Dakota Mahan and Louis Castricato and Jan-Philipp Franken and Nick Haber and Chelsea Finn},
 journal = {arXiv preprint arXiv:2501.04682},
 year = {2025},
 title = {Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Though},
 url={https://arxiv.org/abs/2501.04682},
 google_scholar_id={bFI3QPDXJZMC}, 
 bibtex_show={true},
 selected={true},
 preview={xiang_towards_2025.png},
}

@article{majumder_clin_2024,
 abstract = {Language agents have shown some ability to interact with an external environment, eg, a virtual world such as ScienceWorld, to perform complex tasks, eg, growing a plant, without the startup costs of reinforcement learning. While recent work, eg, Reflexion, has demonstrated how such agents can also self-improve by adding a textual memory of “hints” learned from prior experience, such improvements have been limited both in size and scope. In contrast, our goal is a language agent that can robustly improve performance over time, including when both the task and environment are varied. Our approach is to have the agent learn a textual representation of how the world works (rather than just isolated hints), expressed as a memory of causal abstractions, to guide future decision-making. In experiments, we find CLIN is able to continually improve on repeated trials on the same task and environment, outperforming state-of-the-art reflective language agents like Reflexion by 23 points in ScienceWorld and 1.4 points in ALFWorld benchmarks. CLIN can also transfer its learning to new environments and tasks, enhancing performance by 21 points in ScienceWorld and 11 points in ALFWorld. This suggests that language agents with a textual causal memory can play a significant role in interactive environments, including being able to rapidly improve over time.},
 author = {Bodhisattwa Prasad Majumder and Bhavana Dalvi Mishra and Peter Jansen and Oyvind Tafjord and Niket Tandon and Li Zhang and Chris Callison-Burch and Peter Clark and Ajay Patel and Colin Raffel and Chris Callison-Burch and Andrew Zhu and Alyssa Hwang and Liam Dugan and Chris Callison-Burch and Liam Dugan and Alyssa Hwang and Filip Trhlik and Josh Magnus Ludan and Andrew Zhu and Hainiu Xu and Daphne Ippolito and Chris Callison-Burch and Yue Yang and Fan-Yun Sun and Luca Weihs and Eli VanderBilt and Alvaro Herrasti and Winson Han and Jiajun Wu and Nick Haber and Ranjay Krishna and Lingjie Liu and Chris Callison-Burch and Mark Yatskar and Aniruddha Kembhavi and Christopher Clark and Yiming Huang and Weilin Wan and Yue Yang and Chris Callison-Burch and Mark Yatskar and Lingjie Liu and Andrew Zhu and Liam Dugan and Chris Callison-Burch and Yue Yang and Mona Gandhi and Yufei Wang and Yifan Wu and Michael S Yao and Chris Callison-Burch and James C Gee and Mark Yatskar and Artemis Panagopoulou and Coby Melkin and Chris Callison-Burch and Tianyi Zhang and Li Zhang and Zhaoyi Hou and Ziyu Wang and Yuling Gu and Peter Clark and Chris Callison-Burch and Niket Tandon and Li Zhang and Peter Jansen and Tianyi Zhang and Peter Clark and Chris Callison-Burch and Niket Tandon and Jinqi Luo and Tianjiao Ding and Kwan Ho Ryan Chan and Darshan Thaker and Aditya Chattopadhyay and Chris Callison-Burch and René Vidal},

 journal = {Conference on Language Modeling ({COLM})},
 year = {2024},
 publisher = {IEEE/CVF},
 title = {{CLIN}: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization},
}


@article{martinez_measuring_2023,
 abstract = {Humans are interactive agents driven to seek out situations with interesting physical dynamics. Here we formalize the functional form of physical intrinsic motivation. We first collect ratings of how interesting humans find a variety of physics scenarios. We then model human interestingness responses by implementing various hypotheses of intrinsic motivation including models that rely on simple scene features to models that depend on forward physics prediction. We find that the single best predictor of human responses is adversarial reward, a model derived from physical prediction loss. We also find that simple scene feature models do not generalize their prediction of human responses across all scenarios. Finally, linearly combining the adversarial model with the number of collisions in a scene leads to the greatest improvement in predictivity of human responses, suggesting humans are driven towards scenarios that result in high information gain and physical activity.},
 author = {Julio Martinez and Felix Binder and Haoliang Wang and Nick Haber and Judith Fan and Daniel LK Yamins},

 journal = {arXiv preprint arXiv:2305.13452},
 year = {2023},
 title = {Measuring and Modeling Physical Intrinsic Motivation}
}


@article{doyle_developmental_2023,
 abstract = {Infants explore their complex physical and social environment in an organized way. To gain insight into what intrinsic motivations may help structure this exploration, we create a virtual infant agent and place it in a developmentally-inspired 3D environment with no external rewards. The environment has a virtual caregiver agent with the capability to interact contingently with the infant agent in ways that resemble play. We test intrinsic reward functions that are similar to motivations that have been proposed to drive exploration in humans: surprise, uncertainty, novelty, and learning progress. These generic reward functions lead the infant agent to explore its environment and discover the contingencies that are embedded into the caregiver agent. The reward functions that are proxies for novelty and uncertainty are the most successful in generating diverse experiences and activating the environment contingencies. We also find that learning a world model in the presence of an attentive caregiver helps the infant agent learn how to predict scenarios with challenging social and physical dynamics. Taken together, our findings provide insight into how curiosity-like intrinsic rewards and contingent social interaction lead to dynamic social behavior and the creation of a robust predictive world model.},
 author = {Chris Doyle and Sarah Shader and Michelle Lau and Megumi Sano and Daniel LK Yamins and Nick Haber},

 journal = {arXiv preprint arXiv:2305.13396},
 year = {2023},
 title = {Developmental curiosity and social interaction in virtual agents}
}


@article{wang_scaffold_2024,
 abstract = {Developing problem-solving competency is central to Science, Technology, Engineering, and Mathematics (STEM) education, yet translating this priority into effective approaches to problem-solving instruction and assessment remain a significant challenge. The recent proliferation of generative artificial intelligence (genAI) tools like ChatGPT in higher education introduces new considerations about how these tools can help or hinder students' development of STEM problem-solving competency. Our research examines these considerations by studying how and why college students use genAI tools in their STEM coursework, focusing on their problem-solving support. We surveyed 40 STEM college students from diverse U.S. institutions and 28 STEM faculty to understand instructor perspectives on effective genAI tool use and guidance in STEM courses. Our findings reveal high adoption rates and diverse applications of genAI tools among STEM students. The most common use cases include finding explanations, exploring related topics, summarizing readings, and helping with problem-set questions. The primary motivation for using genAI tools was to save time. Moreover, over half of student participants reported simply inputting problems for AI to generate solutions, potentially bypassing their own problem-solving processes. These findings indicate that despite high adoption rates, students' current approaches to utilizing genAI tools often fall short in enhancing their own STEM problem-solving competencies. The study also explored students' and STEM instructors' perceptions of the benefits and risks associated with using genAI tools in STEM …},
 author = {Karen D Wang and Zhangyang Wu and L'Nard Tufts II and Carl Wieman and Shima Salehi and Nick Haber},

 journal = {arXiv preprint arXiv:2412.02653},
 year = {2024},
 title = {Scaffold or Crutch? Examining College Students' Use and Views of Generative AI Tools for STEM Education}
}


@article{haber_curiosity_2022,
 abstract = {If we were to distill the learning that we see in a child’s playroom into a computer program, how would we? We might start by describing essential properties–the “engineering specifications” of childhood learning. Early childhood learning is incredibly interactive (Fantz 1964; Gopnik et al. 1999; Begus et al. 2014; Goupil et al. 2016; Twomey and Westermann 2018). Children play, grabbing and manipulating objects, learning about the properties and affordances of their worlds. Their learning is both autonomous and social. They engage in incredibly complex self-play, yet they also learn from demonstration and imitation (Tomasello et al. 1993; Tomasello 2016). Further, their behavior is curiosity-driven, satisfying not only instrumental needs, but also intrinsic motivations to understand and control (Kidd et al. 2012; Dweck 2017). In engaging in these activities, they build powerful, general representations about their worlds, including those that give them a sense of intuitive physics (Spelke 1985) and intuitive psychology (Colle et al. 2007; Woodward 2009).},
 author = {Nick Haber},

 pages = {37-54},
 year = {2022},
 publisher = {Springer International Publishing},
 title = {Curiosity and Interactive Learning in Artificial Systems}
}


@article{kline_superpower_2020,
 author = {Aaron Kline and Michael Ning and Arman Husic and Peter Washington and Catalin Voss and Kaitlyn L Dunlap and Yordan Penev and Emilie Leblanc and Nick Haber and Dennis Wall},

 journal = {INSAR 2020 Virtual Meeting},
 year = {2020},
 publisher = {INSAR},
 title = {superpower glass: An augmented reality intervention for improving social deficits in children with autism spectrum disorder}
}


@article{haber_modeling_2018,
 abstract = {Deep convolutional neural networks, when trained on difficult supervised tasks such as object classification on large datasets, have shown the remarkable property of learning general representations useful for many other tasks and predictive of responses in the ventral visual stream. These successes have led to the pursuit of tasks that are able to generate a visual backbone in developmentally-realistic ways. Yet large gaps remain to be filled.-Lack of supervision Training must happen without large amounts of manually-labeled data.-Interaction and agency The developmental behavioral literature intertwines such developments with interaction with the world, explaining rich behaviors through this training. We built a simulated environment based on a game engine that provides 3D visual stimuli and allows for agent interactions with different types of objects. In this, we have an artificial agent gather experience, and …},
 author = {Nick Haber and Damian Mrowca and Li Fei-Fei and Daniel Yamins},

 journal = {Journal of Vision},
 number = {10},
 pages = {10-10},
 year = {2018},
 publisher = {The Association for Research in Vision and Ophthalmology},
 title = {Modeling the scientist in the crib},
 volume = {18}
}


@article{wang_discovering_2024,
 abstract = { Digital games offer promising platforms for assessing student higher-order competencies such as problem-solving. However, processing and analyzing the large volume of interaction log data generated in these platforms to uncover meaningful behavioral patterns remain a complex research challenge. In this study, we employ sequence mining and clustering techniques to examine students’ log data in an interactive puzzle game that requires player to change rules to win the game. Our goal is to identify behavioral characteristics associated with the problem-solving practices adopted by individual students. The findings indicate that the most effective problem solvers made fewer rule changes and took longer time to make those changes across both an introductory and a more advanced level of the game. Conversely, rapid rule change actions were linked to ineffective problem-solving. This research underscores …},
 author = {Karen D Wang and Haoyu Liu and David DeLiema and Nick Haber and Shima Salehi},

 pages = {498-506},
 year = {2024},
 title = {Discovering Players’ Problem-Solving Behavioral Characteristics in a Puzzle Game through Sequence Mining}
}


@article{sano_learning_2020,
 abstract = {From an early age, humans are capable of learning about theirsocial environment, making predictions of how other agentswill operate and decisions about how they themselves will in-teract. In this work, we address the problem of formalizing thelearning principles underlying these abilities. We construct a cu-rious neural agent that can efficiently learn predictive models ofsocial environments that are rich with external agents inspiredby real-world animate behaviors such as peekaboo, chasing,and mimicry. Our curious neural agent consists of a controllerdriven by γ-Progress, a scalable and effective curiosity signal,and a disentangled world model that allocates separate networksfor interdependent components of the world. We show that ourdisentangled curiosity-driven agent achieves higher learning ef-ficiency and prediction performance than strong baselines. Cru-cially, we find that a preference for animate attention emergesnaturally in our model, and is a key driver of performance. Fi-nally we discuss future directions including applications of ourframework to modeling human behavior and designing earlyindicators for developmental variability.},
 author = {Megumi Sano and Julian De Freitas and Nick Haber and Daniel LK Yamins},

 journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
 year = {2020},
 title = {Learning in social environments with curious neural agents},
 volume = {42}
}


@article{haber_microlocal_2013,
 abstract = {Microlocal analysis relies on correspondences between quantum physics and classical physics to give information about certain PDEs--for instance, linear variable-coefficient PDEs on manifolds. PDEs are interpreted as quantum systems. The corresponding classical systems tell us, for example, function spaces on which problems are solvable or almost solvable, existence and uniqueness results, and the structure of solution operators. Landmark papers of Hörmander and Duistermaat and Hörmander establish key results for the standard calculus of microlocal analysis, which gives a broad framework for dealing with variable-coefficient PDEs on manifolds. Their work is well-suited for dealing with PDEs which, in a generalized sense, are hyperbolic, with corresponding classical dynamics looking like wave propagation of geometric optics. In this thesis, we aim to extend many of their results to situations in which the …},
 author = {Nick Haber},

 year = {2013},
 title = {Microlocal analysis of Lagrangian submanifolds of radial points}
}


@article{doyle_intrinsically_2023,
 abstract = {Infants explore their complex physical and social environment in an organized way. To gain insight into what intrinsic motivations may help structure this exploration, we create a virtual infant agent and place it in a developmentally-inspired 3D environment with no external rewards. The environment has a virtual caregiver agent with the capability to interact contingently with the infant agent in ways that resemble play. We test intrinsic reward functions that are similar to motivations that have been proposed to drive exploration in humans: surprise, uncertainty, novelty, and learning progress. The reward functions that are proxies for novelty and uncertainty are the most successful in generating diverse experiences and activating the environment contingencies. We also find that learning a world model in the presence of an attentive caregiver helps the infant agent learn how to predict scenarios with challenging social and physical dynamics. Our findings provide insight into how curiosity-like intrinsic rewards and contingent social interaction lead to social behavior and the creation of a robust predictive world model.},
 author = {Chris Doyle and Sarah Shader and Michelle Lau and Megumi Sano and Daniel Yamins and Nick Haber},
 year= {2023},
 conference = {Intrinsically-Motivated and Open-Ended Learning Workshop@ NeurIPS2023},
 title = {Intrinsically Motivated Social Play in Virtual Infants}
}


@article{hutson_policy-shaped_2024,
 abstract = {Model-based reinforcement learning (MBRL) is a promising route to sample-efficient policy optimization. However, a known vulnerability of reconstruction-based MBRL consists of scenarios in which detailed aspects of the world are highly predictable, but irrelevant to learning a good policy. Such scenarios can lead the model to exhaust its capacity on meaningless content, at the cost of neglecting important environment dynamics. While existing approaches attempt to solve this problem, we highlight its continuing impact on leading MBRL methods -- including DreamerV3 and DreamerPro -- with a novel environment where background distractions are intricate, predictable, and useless for planning future actions. To address this challenge we develop a method for focusing the capacity of the world model through synergy of a pretrained segmentation model, a task-aware reconstruction loss, and adversarial learning. Our method outperforms a variety of other approaches designed to reduce the impact of distractors, and is an advance towards robust model-based reinforcement learning.},
 author = {Miles Hutson and Isaac Kauvar and Nick Haber},

 journal = {arXiv preprint arXiv:2412.05766},
 year = {2024},
 title = {Policy-shaped prediction: avoiding distractions in model-based reinforcement learning}
}


@article{sun_layoutvlm:_2024,
 abstract = {Open-universe 3D layout generation arranges unlabeled 3D assets conditioned on language instruction. Large language models (LLMs) struggle with generating physically plausible 3D scenes and adherence to input instructions, particularly in cluttered scenes. We introduce LayoutVLM, a framework and scene layout representation that exploits the semantic knowledge of Vision-Language Models (VLMs) and supports differentiable optimization to ensure physical plausibility. LayoutVLM employs VLMs to generate two mutually reinforcing representations from visually marked images, and a self-consistent decoding process to improve VLMs spatial planning. Our experiments show that LayoutVLM addresses the limitations of existing LLM and constraint-based approaches, producing physically plausible 3D layouts better aligned with the semantic intent of input language instructions. We also demonstrate that fine-tuning VLMs with the proposed scene layout representation extracted from existing scene datasets can improve performance.},
 author = {Fan-Yun Sun and Weiyu Liu and Siyi Gu and Dylan Lim and Goutam Bhat and Federico Tombari and Manling Li and Nick Haber and Jiajun Wu},

 journal = {arXiv preprint arXiv:2412.02193},
 year = {2024},
 title = {LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language Models}
}


@article{klieger_chatcollab:_2024,
 abstract = {We explore the potential for productive team-based collaboration between humans and Artificial Intelligence (AI) by presenting and conducting initial tests with a general framework that enables multiple human and AI agents to work together as peers. ChatCollab's novel architecture allows agents - human or AI - to join collaborations in any role, autonomously engage in tasks and communication within Slack, and remain agnostic to whether their collaborators are human or AI. Using software engineering as a case study, we find that our AI agents successfully identify their roles and responsibilities, coordinate with other agents, and await requested inputs or deliverables before proceeding. In relation to three prior multi-agent AI systems for software development, we find ChatCollab AI agents produce comparable or better software in an interactive game development task. We also propose an automated method for analyzing collaboration dynamics that effectively identifies behavioral characteristics of agents with distinct roles, allowing us to quantitatively compare collaboration dynamics in a range of experimental conditions. For example, in comparing ChatCollab AI agents, we find that an AI CEO agent generally provides suggestions 2-4 times more often than an AI product manager or AI developer, suggesting agents within ChatCollab can meaningfully adopt differentiated collaborative roles. Our code and data can be found at: https://github.com/ChatCollab.},
 author = {Benjamin Klieger and Charis Charitsis and Miroslav Suzara and Sierra Wang and Nick Haber and John C Mitchell},

 journal = {arXiv preprint arXiv:2412.01992},
 year = {2024},
 title = {ChatCollab: Exploring Collaboration Between Humans and AI Agents in Software Teams}
}


@article{sun_factorsim:_2024,
 abstract = {Generating simulations to train intelligent agents in game-playing and robotics from natural language input, user input, or task documentation remains an open-ended challenge. Existing approaches focus on parts of this challenge, such as generating reward functions or task hyperparameters. Unlike previous work, we introduce FACTORSIM that generates full simulations in code from language input that can be used to train agents. Exploiting the structural modularity specific to coded simulations, we propose to use a factored partially observable Markov decision process representation that allows us to reduce context dependence during each step of the generation. For evaluation, we introduce a generative simulation benchmark that assesses the generated simulation code’s accuracy and effectiveness in facilitating zero-shot transfers in reinforcement learning settings. We show that FACTORSIM outperforms existing methods in generating simulations regarding prompt alignment (i.e., accuracy), zero-shot transfer abilities, and human evaluation. We also demonstrate its effectiveness in generating robotic tasks.},
 author = {Fan-Yun Sun and SI Harini and Angela Yi and Yihan Zhou and Alex Zook and Jonathan Tremblay and Logan Cross and Jiajun Wu and Nick Haber},

 conference = {The Thirty-eighth Annual Conference on Neural Information Processing Systems},
 year = {2024},
 title = {FactorSim: Generative Simulation via Factorized Representation}
}


@article{li_tutorly:_2024,
 abstract = {Online programming videos, including tutorials and streamcasts, are widely popular and contain a wealth of expert knowledge. However, effectively utilizing these resources to achieve targeted learning goals can be challenging. Unlike direct tutoring, video content lacks tailored guidance based on individual learning paces, personalized feedback, and interactive engagement necessary for support and monitoring. Our work transforms programming videos into one-on-one tutoring experiences using the cognitive apprenticeship framework. Tutorly, developed as a JupyterLab Plugin, allows learners to (1) set personalized learning goals, (2) engage in learning-by-doing through a conversational LLM-based mentor agent, (3) receive guidance and feedback based on a student model that steers the mentor moves. In a within-subject study with 16 participants learning exploratory data analysis from a streamcast, Tutorly significantly improved their performance from 61.9% to 76.6% based on a post-test questionnaire. Tutorly demonstrates the potential for enhancing programming video learning experiences with LLM and learner modeling.},
 author = {Wengxi Li and Roy Pea and Nick Haber and Hari Subramonyam},

 journal = {arXiv preprint arXiv:2405.12946},
 year = {2024},
 title = {Tutorly: Turning Programming Videos Into Apprenticeship Learning Environments with LLMs}
}


@article{sun_partial-view_2024,
 abstract = {We propose Filtering Inversion (FINV), a learning framework and optimization process that predicts a renderable 3D object representation from one or few partial views. FINV addresses the challenge of synthesizing novel views of objects from partial observations, spanning cases where the object is not entirely in view, is partially occluded, or is only observed from similar views. To achieve this, FINV learns shape priors by training a 3D generative model. At inference, given one or more views of a novel real-world object, FINV first finds a set of latent codes for the object by inverting the generative model from multiple initial seeds. Maintaining the set of latent codes, FINV filters and resamples them after receiving each new observation, akin to particle filtering. The generator is then finetuned for each latent code on the available views in order to adapt to novel objects. We show that FINV successfully synthesizes novel …},
 author = {Fan-Yun Sun and Jonathan Tremblay and Valts Blukis and Kevin Lin and Danfei Xu and Boris Ivanovic and Peter Karkus and Stan Birchfield and Dieter Fox and Ruohan Zhang and Yunzhu Li and Jiajun Wu and Marco Pavone and Nick Haber},

 conference = {2024 International Conference on 3D Vision (3DV)},
 pages = {453-463},
 year = {2024},
 publisher = {IEEE},
 title = {Partial-View Object View Synthesis via Filtering Inversion}
}


@article{wang_math_2024,
 abstract = {To inspire student engagement in middle school math, we explore the possibility of using generative AI to enhance the creativity of math learning. We present the Math IDE, a math education environment in which students learn about math concepts by building artifacts. We aimed to create a platform in which students can engage with mathematical concepts, create an artifact that embodies the math that they are learning about, and practice their high-level specification skills. In the current iteration of the Math IDE, students can create custom web pages by describing and demonstrating understanding of the math that is involved in the web page. In this short overview, we describe our process and discuss several open questions regarding the design and application of this novel method of math education.},
 author = {Sierra Wang and John Mitchell and Nick Haber and Chris Piech},

 pages = {1844-1845},
 year = {2024},
 title = {Math IDE: A Platform for Creating with Math}
}


@article{cross_animate_2024,
 abstract = {To advance the capacity of intuitive psychology in machines, we introduce the Animate Agent World Modeling Benchmark. This benchmark features agents engaged in a diverse repertoire of behaviors, such as goal-directed interactions with objects and multi-agent interactions, all governed by realistic physics. Humans tend to predict the future based on expected events rather than simulating step-by-step. Thus, our benchmark includes a cognitively-inspired evaluation pipeline designed to assess whether the simulated trajectories of world models capture the correct sequences of events. To perform well, models need to leverage predictive cues from the observations to accurately simulate the goals of animate agents over long horizons. We demonstrate that current state-of-the-art models perform poorly in our evaluations. A hierarchical oracle model sets an upper bound for performance, suggesting that to excel, a model should scaffold their predictions with abstractions like goals that guide the simulation process towards relevant future events},
 author = {Logan Matthew Cross and Violet Xiang and Nick Haber and Daniel Yamins},

 journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
 year = {2024},
 title = {Animate Agent World Modeling Benchmark},
 volume = {46}
}


@article{zhou_simulating_2024,
 abstract = {Attachment is crucial for infants' cognitive development and social relationships. Traditional attachment research has been qualitative, lacking a model to explain how infants' attachment styles develop from experience and how these are influenced by personal traits and environmental factors. We propose such a model, predicting how infants balance interaction with caregivers against exploring their surroundings. Our study is based in a grid-world environment containing an infant and caregiver agent. We vary the infant’s temperamental factors (e.g., ability to regulate emotions and preferences for social vs. environmental reward), and caregiver behavior (whether positive or negative interactions are more likely). We find that different equilibria result that qualitatively correspond to different attachment styles. Our findings suggest that the characteristic exploratory behavior of each attachment style in real infants may arise from interactions of infant temperament and caregiver behaviors.},
 author = {Xi Jia Zhou and Chris Doyle and Michael C Frank and Nick Haber},

 journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
 year = {2024},
 title = {Simulating Infants’ Attachment: Behavioral Patterns of Caregiver Proximity Seeking and Environment Exploration Using Reinforcement Learning Models.},
 volume = {46}
}


@article{martinez_modeling_2023,
 abstract = {Humans are efficient social learners who leverage social information to rapidly adapt to new environments, but the computations by which we combine social information with prior knowledge are poorly understood. We study social learning within the context of multi-armed bandits using a novel “asteroid mining” video game where participants learn through active play and passive observation of expert and novice players. We simulate human exploration and social learning using naive versions of Thompson and Upper Confidence Bound (UCB) solvers and hybrid models that use Thompson and UCB solvers for direct learning together with a multi-layer perceptron to estimate what should be learned from other players. Two variants of the hybrid models provide good, parameter-free fits to human performance across a range of learning conditions. Our work shows a route for integrating social learning into reinforcement learning models and suggests that human social learning conforms to the predictions of such models.},
 author = {Julio Martinez and Michael C Frank and Nick Haber},

 journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
 year = {2023},
 title = {Modeling Social Learning Through Demonstration in Multi-Armed Bandits},
 volume = {46}
}


@article{xiang_from_2023,
 abstract = {In real-world environments, autonomous agents rely on their egocentric observations. They must learn adaptive strategies to interact with others who possess mixed motivations, discernible only through visible cues. Several Multi-Agent Reinforcement Learning (MARL) methods adopt centralized approaches that involve either centralized training or reward-sharing, often violating the realistic ways in which living organisms, like animals or humans, process information and interact. MARL strategies deploying decentralized training with intrinsic motivation offer a self-supervised approach, enable agents to develop flexible social strategies through the interaction of autonomous agents. However, by contrasting the self-supervised and centralized methods, we reveal that populations trained with reward-sharing methods surpass those using self-supervised methods in a mixed-motive environment. We link this superiority to specialized role emergence and an agent's expertise in its role. Interestingly, this gap shrinks in pure-motive settings, emphasizing the need for evaluations in more complex, realistic environments (mixed-motive). Our preliminary results suggest a gap in population performance that can be closed by improving self-supervised methods and thereby pushing MARL closer to real-world readiness.},
 author = {Violet Xiang and Logan Cross and Jan-Philipp Fränken and Nick Haber},

 journal = {arXiv preprint arXiv:2312.08662},
 year = {2023},
 title = {From Centralized to Self-Supervised: Pursuing Realistic Multi-Agent Reinforcement Learning}
}


@article{cerit_tingets_2023,
 abstract = {Young children between the ages of 3 and 6 often struggle with identifying, expressing, and managing their emotions, especially when it comes to negative emotions. Research indicates that children with higher emotional skills tend to experience greater long-term mental health and well-being benefits. Building on our previous work, we introduce Tingets 2.0, an interactive storytelling app powered by computer vision to facilitate social and emotional learning (SEL) through tangible and digital play. We propose the TinkeRERR framework: Reflect, Express, Regulate, and Repeat, and utilize computer vision technology to combine tangible and digital play seamlessly. Tingets 2.0 aims to help children understand their own and others’ emotions, express emotions constructively, manage negative emotions and cultivate positive emotions. Early results indicate that our novel approach has effectively increased the depth …},
 author = {Merve Cerit and Daniela Vainer and Nick Haber},

 pages = {53-59},
 year = {2023},
 title = {Tingets 2.0: Computer Vision-Powered Interactive Social and Emotional Learning Tool}
}


@article{sun_partial-view_2023,
 abstract = {We propose Filtering Inversion (FINV), a learning framework and optimization process that predicts a renderable 3D object representation from one or few partial views. FINV addresses the challenge of synthesizing novel views of objects from partial observations, spanning cases where the object is not entirely in view, is partially occluded, or is only observed from similar views. To achieve this, FINV learns shape priors by training a 3D generative model. At inference, given one or more views of a novel real-world object, FINV first finds a set of latent codes for the object by inverting the generative model from multiple initial seeds. Maintaining the set of latent codes, FINV filters and resamples them after receiving each new observation, akin to particle filtering. The generator is then finetuned for each latent code on the available views in order to adapt to novel objects. We show that FINV successfully synthesizes novel views of real-world objects (e.g., chairs, tables, and cars), even if the generative prior is trained only on synthetic objects. The ability to address the sim-to-real problem allows FINV to be used for object categories without real-world datasets. FINV achieves state-of-the-art performance on multiple real-world datasets, recovers object shape and texture from partial and sparse views, is robust to occlusion, and is able to incrementally improve its representation with more observations.},
 author = {Fan-Yun Sun and Jonathan Tremblay and Valts Blukis and Kevin Lin and Danfei Xu and Boris Ivanovic and Peter Karkus and Stan Birchfield and Dieter Fox and Ruohan Zhang and Yunzhu Li and Jiajun Wu and Marco Pavone and Nick Haber},

 journal = {arXiv preprint arXiv:2304.00673},
 year = {2023},
 title = {Partial-View Object View Synthesis via Filtered Inversion}
}


@article{zhou_reinforcement_2023,
 abstract = {Infants must balance interacting with a caregiver and navigating the environment, driven by their curiosity and attachment. This decision involves trading off proximity seeking with the caregiver and novelty seeking in the surroundings. Our hypothesis is that different attachment styles may arise as adaptive reward-seeking strategies based on different frequencies and scales of rewards from both caregiver and environment, including negative rewards due to distressing encounters. We implement a computational model using a reinforcement learning framework to model infantsÕ attachment and exploratory behaviors as a bandit problem. We study how infantsÕ exploratory behaviors emerge as they learn from their caregiver and how these learned behaviors transfer to a new caregiver. Our work expands upon existing computational models of attachment and models how social factors can influence curiosity-seeking as infants learn to explore versus exploit.},
 author = {Xi Jia Zhou and Frieda Rong and Michael Frank and Nick Haber},

 journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
 year = {2023},
 title = {Reinforcement learning models of tradeoffs between infant attachment and curiosity}
}


@article{radwan_measuring_2022,
 abstract = {In learning about and building models of their social and physical worlds, children exhibit a wide range of complex, spontaneous, intrinsically motivated, curiosity-driven behaviors. To understand how curiosity-driven behavior in social environments differs in diverse learners, we experimentally measure attention allocation in children on the autism spectrum and their typically-developing peers. We designed an augmented reality smartphone activity where children freely explore novel stimuli. Some agents behave “socially” (i.e., animate), while others have more regular (i.e., periodic, static) or irregular (i.e., random) inanimate behavioral patterns. This project is part of a broader program in which we attempt to model curiosity with artificial intelligence. Our augmented reality systems will lead to large and diverse data acquisition, allowing us to model a comprehensive range of learning behaviors and enable more inclusive, personalized pedagogical and diagnostic tools.},
 author = {Samaher F Radwan and Aaron Kline and Alejandro Galindo and Michael C Frank and Dennis Wall and Nick Haber},

 journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
 number = {44},
 year = {2022},
 title = {Measuring social curiosity-driven attentional differences in children with autism using an augmented reality-based phone app},
 volume = {44}
}


@article{voss_positive_2019,
 author = {Catalin Voss and Nick Haber and Dennis P Wall},

 journal = {JAMA PEDIATRICS},
 number = {11},
 pages = {1106-1106},
 year = {2019},
 publisher = {AMER MEDICAL ASSOC},
 title = {Positive Childhood Experiences and Adult Mental and Relational Health in a Statewide Sample Associations Across Adverse Childhood Experiences Levels In Reply},
 volume = {173}
}


@article{freitas_intrinsic_2019,
 abstract = {Humans naturally pay attention to other animate agents in their environment, a prosocial behavior that has been documented as early as a few weeks. What internal mechanisms give rise to this behavior? A standard hypothesis is that the human brain has a built-in module that specifically detects animacy from visual input. We hypothesize that animate attention naturally arises from a more general process of curiosity driven learning. We ran experiments to measure important features of animate attention. Observers (N= 12, A gerange= 43 years) wore a mobile eye tracker while watching a display consisting of four self-propelled, spherical robots travelling along a mat. Using kinematics alone, the robots were made to appear animate, random, periodic, or static. Average fixations proportions were animate 55%, random 24%, periodic 14%, and static 7%, with 10 of 12 observers fixating most on animate. We also …},
 author = {Julian De Freitas and Kun Ho Kim and Nick Haber and Colin Conwell and George A Alvarez and Daniel LK Yamins},

 journal = {Journal of Vision},
 number = {10},
 pages = {17d-17d},
 year = {2019},
 publisher = {The Association for Research in Vision and Ophthalmology},
 title = {Intrinsic curiosity may give rise to animate attention},
 volume = {19}
}


@article{washington_addendum_2019,
 abstract = {The correction will appear in the online version of the paper on the JMIR website on June 27, 2019, together with the publication of this correction notice. Because this was made after submission to PubMed, PubMed Central, and other full-text repositories, the corrected article also has been resubmitted to those repositories.},
 author = {Peter Washington and Haik Kalantarian and Qandeel Tariq and Jessey Schwartz and Kaitlyn Dunlap and Brianna Chrisman and Maya Varma and Michael Ning and Aaron Kline and Nathaniel Stockham and Kelley Paskov and Catalin Voss and Nick Haber and Dennis Paul Wall},

 journal = {Journal of Medical Internet Research},
 number = {6},
 pages = {e14950},
 year = {2019},
 publisher = {JMIR Publications},
 title = {Addendum to the Acknowledgements: Validity of Online Screening for Autism: Crowdsourcing Study Comparing Paid and Unpaid Diagnostic Tasks},
 volume = {21}
}


@article{semizer_to_2018,
 abstract = {Volume 18 Issue 10 | JOV | ARVO Journals jov Issues Topics For Authors About Editorial Board 
Journals Home MANAGE ALERTS Access Provided By: Google Indexer Forgot password? 
Create an Account To View More... Purchase this article with an account. or Subscribe Now 
AdvancedSearch Issues Topics For Authors About Editorial Board Current Issue Past Issues 
Special Issues/Collections Select a back issue 2023 2022 2021 2020 2019 2018 2017 2016 
2015 2014 2013 2012 2011 2010 2009 2008 2007 2006 2005 2004 2003 2002 2001 Select a 
back issue June 2023 Volume 23 Issue 6 Articles 1 - 13 May 2023 Volume 23 Issue 5 Articles 
1 - 21 April 2023 Volume 23 Issue 4 Articles 1 - 8 March 2023 Volume 23 Issue 3 Articles 
1 - 19 February 2023 Volume 23 Issue 2 Articles 1 - 13 January 2023 Volume 23 Issue 1 
Articles 1 - 17 December 2022 Volume 22 Issue 14 Articles 3038 - 4516 December 2022 …},
 author = {Yelda Semizer and Melchi Michel and Karla Evans and Jeremy Wolfe and Olivier Penacchio and Julie Harris and Aditya Jonnalagadda and Arturo Deza and Miguel Eckstein and Patrick Cox and Dwight Kravitz and Stephen Mitroff and Taylor Hayes and John Henderson and Hayden Schill and Farahnaz Wick and Matthew Cain and Ian Donovan and Marisa Carrasco},

 journal = {Journal of Vision},
 pages = {1},
 year = {2018},
 title = {To View More...},
 volume = {18}
}


@article{calvo_invited_2016,
 abstract = {This paper contains abstracts of the invited talks presented at the AAAI 2016 Symposium on Well-Being Computing.},
 author = {Rafael A Calvo and Nick Haber and Catalin Voss and Michael Nova and Dennis Salins and Michael Snyder and Dennis P Wall},

 conference = {2016 AAAI Spring Symposium Series},
 year = {2016},
 title = {Invited Talks at the AAAI Symposium on Well-Being Computing}
}



@article{truong_nonmyopic_2024,
 abstract = {Bayesian optimization (BO) is a popular framework for optimizing black-box functions, leveraging probabilistic models such as Gaussian processes. However, conventional BO assumes static query costs, which limits its applicability to real-world problems with dynamic cost structures, such as geological surveys or biological sequence design, where query costs vary based on previous actions. To address this, we propose a cost-constrained nonmyopic BO algorithm that incorporates dynamic cost models. Our method employs a neural network policy for variational optimization over multi-step lookahead horizons to plan ahead in dynamic cost environments. Empirically, we benchmark our method on synthetic functions exhibiting a variety of dynamic cost structures. Furthermore, we apply our method to a real-world application in protein sequence design using a large language model-based policy, demonstrating its scalability and effectiveness in handling multi-step planning in a large and complex query space. Our nonmyopic BO algorithm consistently outperforms its myopic counterparts in both synthetic and real-world settings, achieving significant improvements in both efficiency and solution quality.},
 year = {2024},
 author = {Sang T Truong and Duc Quang Nguyen and Willie Neiswanger and Ryan-Rhys Griffiths and Stefano Ermon and Nick Haber and Sanmi Koyejo},
 title = {Nonmyopic Bayesian Optimization in Dynamic Cost Settings}
}


@article{cerit_person-specific_2024,
 abstract = {Contrary to popular concerns about the harmful effects of media use on mental health, research on this relationship is ambiguous, stalling advances in theory, interventions, and policy. Scientific explorations of the relationship between media and mental health have mostly found null or small associations, with the results often blamed on the use of cross-sectional study designs or imprecise measures of media use and mental health.This exploratory empirical demonstration aimed to answer whether mental health effects are associated with media use experiences by (1) redirecting research investments to granular and intensive longitudinal recordings of digital experiences to build models of media use and mental health for single individuals over the course of one entire year, (2) using new metrics of fragmented media use to propose explanations of mental health effects that will advance …},
 author = {Merve Cerit and Angela Y Lee and Jeffrey Hancock and Adam Miner and Mu-Jung Cho and Daniel Muise and Anna-Angelina Garròn Torres and Nick Haber and Nilam Ram and Thomas Robinson and Byron Reeves},
 year = {2024},
 journal = {JMIR formative research},
 title = {Person-Specific Analyses of Smartphone Use and Mental Health: An Intensive Longitudinal Study Over One Year}
}


@article{hutson_policy-shaped_2024,
 abstract = {Model-based reinforcement learning (MBRL) offers sample-efficient policy optimization but is susceptible to distractions. We address this by developing Policy-Shaped Prediction (PSP), a method that empowers agents to interpret their own policies and shape their world models accordingly. By combining gradient-based interpretability, pretrained segmentation models, and adversarial learning, PSP outperforms existing distractor-reduction approaches. This work represents an interpretability-driven advance towards robust MBRL.},
 author = {Miles Richard Hutson and Isaac Kauvar and Nick Haber},
 year = {2024},
 conference = {Interpretable AI: Past, Present and Future},
 title = {Policy-shaped prediction: improving world modeling through interpretability}
}


@article{zhou_does_2024,
 abstract = {Infant attachment behaviors, essential for cognitive and social development, are debated in terms of their origins—whether they arise from intrinsic drives or external reinforcement. This study uses a reinforcement learning (RL) framework to examine if infant attachment behaviors can be replicated in a simulated 1D grid world environment. Two groups of simulated infants were modeled: one with intrinsic rewards for staying close to a caregiver, and another relying solely on extrinsic rewards from both the caregiver and the environment. Preliminary results show that external reinforcement can generate infant-like attachment behaviors, though adding intrinsic attachment rewards made some overall infant behaviors more realistic while distorting subgroup patterns. These findings highlight the complex interaction between intrinsic and extrinsic factors in shaping attachment behaviors, suggesting more work is required …},
 author = {Xi Jia Zhou and Logan Cross and Wanqiao Xu and Nick Haber},
 year = {2024},
 conference = {Intrinsically-Motivated and Open-Ended Learning Workshop@ NeurIPS2024},
 title = {Does Infantile Attachment Require Intrinsic Reward?}
}



@article{kauvar_neurobehavior_2023,
 abstract = {We study intrinsically motivated exploration by artificially intelligent (AI) agents in animal-inspired settings. We construct virtual environments that are 3D, vision-based, physics-simulated, and based on two established animal assays: labyrinth exploration, and novel object interaction. We assess Plan2Explore (P2E), a leading model-based, intrinsically motivated deep reinforcement learning agent, in these environments. We characterize and compare the behavior of the AI agents to animal behavior, using measures devised for animal neuroethology. P2E exhibits some similarities to animal behavior, but is dramatically less efficient than mice at labyrinth exploration. We further characterize the neural dynamics associated with world modeling in the novel-object assay. We identify latent neural population activity axes linearly associated with representing object proximity. These results identify areas of improvement for existing AI agents, and make strides toward understanding the learned neural dynamics that guide their behavior.},
 author = {Isaac Kauvar and Chris Doyle and Nick Haber},
 year = {2023},
 conference = {Intrinsically-Motivated and Open-Ended Learning Workshop@ NeurIPS2023},
 title = {Neurobehavior of exploring AI agents}
}


@article{washington_autism_2019,
 abstract = {Background: Obtaining a diagnosis of neuropsychiatric disorders such as autism requires long waiting times often exceeding a year. Furthermore, obtaining a professional diagnosis is often prohibitively expensive for much of the global population. Crowdsourcing provides a cheap and possibly free alternative that can allow underserved populations to obtain an accurate diagnosis.Objective: To determine whether paid crowd workers on Amazon Mechanical Turk (AMT) as well as citizen crowd workers on a public website shared on social media can provide accurate online screening for autism from watching a short video clip.Methods: Three online studies:(1) a paid crowdsourcing task on AMT (N= 55) where crowd workers were asked to rate ten short video clips of children as having “autism” or “no autism”,(2) a follow-up, more complex paid crowdsourcing task on AMT (N= 27) with only those raters who correctly rated 8 or more of the 10 videos during the first study, and (3) a public unpaid “citizen healthcare” study (N= 71) with the same task as in the first study. 50% of the videos in all studies contained children with autism, 50% of videos contained females, and the age range of children in the videos ranged from 2 to 5 years.Results: Study 1: The mean score of the participants who completed all questions was 7.50 (SD= 1.46) out of 10. When only analyzing the workers who scored reasonably well (scored 8 out of 10 or higher; n= 27 out of 55), there was a weak negative correlation between the time spent rating the videos and the sensitivity (r=-0.44, p= 0.02). Study 2: The mean score of the participants rating new videos was 6.76 (SD= 0.59) out …},
 year = {2019},
 author = {Peter Washington and Haik Kalantarian and Qandeel Tariq and Jessey Schwartz and Kaitlyn Dunlap and Brianna Chrisman and Maya Varma and Michael Ning and Aaron Kline and Nathaniel Stockham and Kelley Paskov and Catalin Voss and Nick Haber and Dennis Paul Wall},
 title = {Autism Insight Score (AIS): A Crowdsourced Study of Paid and Unpaid â Citizen Healthcareâ}
}


@article{washington_improved_2020,
 abstract = {Automated emotion classification could aid those who struggle to recognize emotion, including children with developmental behavioral conditions such as autism. However, most computer vision emotion recognition models are trained on adult affect and therefore underperform when used on child faces. In this study, we designed a strategy to gamify the collection and the labeling of child affect data in an effort to boost the performance of automatic child emotion detection to a level closer to what will be needed for digital healthcare approaches. We leveraged our prototype therapeutic smartphone game, GuessWhat, which was designed in large part for children with developmental and behavioral conditions, to gamify the secure collection of video data of children expressing a variety of emotions prompted by the game. Independently, we created a secure web interface to gamify the human labeling effort HollywoodSquares, tailored for use by any qualified labeler. We gathered and labeled 2,155 videos, 39,968 emotion frames, and 106,001 labels on all images. With this drastically expanded pediatric emotion centric database (> 30x larger than existing public pediatric affect datasets), we trained a pediatric emotion classification convolutional neural network (CNN) classifier of happy, sad, surprised, fearful, angry, disgust, and neutral expressions in children. The classifier achieved 66.9% balanced accuracy and 67.4% F1-score on the entirety of CAFE as well as 79.1% balanced accuracy and 78.0% F1-score on CAFE Subset A, a subset containing at least 60% human agreement on emotions labels. This performance is at least 10% higher than all …},
 author = {Peter Washington and Haik Kalantarian and Jack Kent and Arman Husic and Aaron Kline and Emilie Leblanc and Cathy Hou and Cezmi Mutlu and Kaitlyn Dunlap and Yordan Penev and Maya Varma and Nate Stockham and Brianna Chrisman and Kelley Paskov and Min Woo Sun and Jae-Yoon Jung and Catalin Voss and Nick Haber and Dennis Paul Wall},
 year={2020},
 title = {Improved Digital Therapy for Developmental Pediatrics Using Domain-Specific Artificial Intelligence}
}


@article{kim_towards_2020,
 abstract = {Children exhibit extraordinary exploratory behaviors hypothesized to contribute to the building of models of their world. Harnessing this capacity in artificial systems promises not only more flexible technology but also cognitive models of the developmental processes we seek to mimic. Yet not all children learn the same way, and for instance children with autism exhibit characteristically different exploratory strategies early in life. What if we could, by developing artificial systems that learn through exploration, model not only typically development, but all its variations? In this work, we present a preliminary analysis of curiosity-driven agents in social environments that establishes links between early behavior and later acuity, with implications for the future of both diagnostics and personalized learning.},
 author = {Kuno Kim and Megumi Sano and Julian De Freitas and Daniel LK Yamins and Nick Haber},
 title = {TOWARDS MODELING THE DEVELOPMENTAL VARIABIL-ITY OF HUMAN ATTENTION},
 year={2020},
}


@article{kalantarian_mobile_2022,
 abstract = {Autism Spectrum Disorder (ASD) is a condition affecting 70 million children worldwide. Due to delays in diagnosis and imbalances in coverage, it is necessary to develop new methods of care delivery for use outside of clinical settings. In this paper, we present a mobile charades-style game, Guess What?, used for crowdsourcing the acquisition of structured video from children with ASD for behavioral disease research. By analyzing facial affect in response to various prompts, we demonstrate that engagement and facial affect can be quantified and measured using real-time image processing algorithms: an important first-step for future therapies, diagnostics, and outcome measures based on home video. Our study of thirteen subjects reveals that game sessions displaying faces to the player produced the most emotive facial expressions in the player by a considerable margin. Videos from the younger neurotypical group contained 73.9% more frames with emotion in this category, compared to the older group with ASD. Moreover, results from all categories indicated 20.7% less facial engagement among older children with ASD compared to younger children with no diagnosis. We will attempt to replicate these findings in a larger cohort in future work.},
 author = {Haik Kalantarian and Nick Haber},
 year = {2022},
 title = {Mobile Crowdsourcing of Emotive Video for Autism Research}
}


